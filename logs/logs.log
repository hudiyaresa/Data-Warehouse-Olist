2024-12-22 04:53:27,609 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 04:53:27,613 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 04:53:27,664 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 04:53:27,680 - ERROR - [pid 14734] Worker Worker(salt=1177210169, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=14734) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5430 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5430 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 04:53:27,764 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 04:53:27,772 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 04:53:27,773 - DEBUG - Asking scheduler for work...
2024-12-22 04:53:27,777 - DEBUG - Done
2024-12-22 04:53:27,777 - DEBUG - There are no more tasks to run at this time
2024-12-22 04:53:27,778 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-22 04:53:27,779 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-22 04:53:27,780 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-22 04:53:27,780 - INFO - Worker Worker(salt=1177210169, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=14734) was stopped. Shutting down Keep-Alive thread
2024-12-22 04:53:27,782 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Load()
    - 1 Transform()
* 1 failed:
    - 1 Extract()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 21:30:29,437 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 21:30:29,442 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 21:30:29,493 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 21:30:29,504 - ERROR - [pid 70529] Worker Worker(salt=3783739673, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=70529) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 21:30:29,553 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 21:30:29,559 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 21:30:29,560 - DEBUG - Asking scheduler for work...
2024-12-22 21:30:29,564 - DEBUG - Done
2024-12-22 21:30:29,564 - DEBUG - There are no more tasks to run at this time
2024-12-22 21:30:29,565 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 21:30:29,565 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 21:30:29,566 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 21:30:29,566 - INFO - Worker Worker(salt=3783739673, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=70529) was stopped. Shutting down Keep-Alive thread
2024-12-22 21:30:29,568 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 21:31:54,817 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 21:31:54,821 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 21:31:54,853 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 21:31:54,863 - ERROR - [pid 70885] Worker Worker(salt=6159693716, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=70885) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 21:31:54,904 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 21:31:54,912 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 21:31:54,913 - DEBUG - Asking scheduler for work...
2024-12-22 21:31:54,916 - DEBUG - Done
2024-12-22 21:31:54,916 - DEBUG - There are no more tasks to run at this time
2024-12-22 21:31:54,917 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 21:31:54,918 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 21:31:54,918 - INFO - Worker Worker(salt=6159693716, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=70885) was stopped. Shutting down Keep-Alive thread
2024-12-22 21:31:54,919 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 21:36:39,390 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 21:36:39,394 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 21:36:39,426 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 21:36:39,435 - ERROR - [pid 72020] Worker Worker(salt=1865527980, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=72020) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 21:36:39,499 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 21:36:39,506 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 21:36:39,506 - DEBUG - Asking scheduler for work...
2024-12-22 21:36:39,509 - DEBUG - Done
2024-12-22 21:36:39,509 - DEBUG - There are no more tasks to run at this time
2024-12-22 21:36:39,510 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 21:36:39,510 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 21:36:39,511 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 21:36:39,511 - INFO - Worker Worker(salt=1865527980, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=72020) was stopped. Shutting down Keep-Alive thread
2024-12-22 21:36:39,512 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 21:49:04,250 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 21:49:04,254 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 21:49:04,306 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 21:49:04,315 - ERROR - [pid 77395] Worker Worker(salt=3111333868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=77395) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 21:49:04,367 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 21:49:04,372 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 21:49:04,372 - DEBUG - Asking scheduler for work...
2024-12-22 21:49:04,374 - DEBUG - Done
2024-12-22 21:49:04,375 - DEBUG - There are no more tasks to run at this time
2024-12-22 21:49:04,375 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 21:49:04,376 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 21:49:04,376 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 21:49:04,377 - INFO - Worker Worker(salt=3111333868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=77395) was stopped. Shutting down Keep-Alive thread
2024-12-22 21:49:04,378 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 21:54:54,907 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 21:54:54,913 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 21:54:54,958 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 21:54:54,968 - ERROR - [pid 78994] Worker Worker(salt=7983099728, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=78994) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 21:54:55,018 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 21:54:55,024 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 21:54:55,025 - DEBUG - Asking scheduler for work...
2024-12-22 21:54:55,027 - DEBUG - Done
2024-12-22 21:54:55,027 - DEBUG - There are no more tasks to run at this time
2024-12-22 21:54:55,028 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 21:54:55,028 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 21:54:55,029 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 21:54:55,029 - INFO - Worker Worker(salt=7983099728, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=78994) was stopped. Shutting down Keep-Alive thread
2024-12-22 21:54:55,030 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 22:27:26,264 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 22:27:26,269 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 22:27:26,306 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 22:27:26,316 - ERROR - [pid 86740] Worker Worker(salt=2129863596, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=86740) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 22:27:26,370 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 22:27:26,375 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 22:27:26,376 - DEBUG - Asking scheduler for work...
2024-12-22 22:27:26,378 - DEBUG - Done
2024-12-22 22:27:26,379 - DEBUG - There are no more tasks to run at this time
2024-12-22 22:27:26,379 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 22:27:26,380 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 22:27:26,380 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 22:27:26,380 - INFO - Worker Worker(salt=2129863596, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=86740) was stopped. Shutting down Keep-Alive thread
2024-12-22 22:27:26,382 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 22:28:54,427 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 22:28:54,432 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 22:28:54,470 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 22:28:54,483 - ERROR - [pid 87106] Worker Worker(salt=5297958816, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=87106) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 22:28:54,533 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 22:28:54,540 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 22:28:54,541 - DEBUG - Asking scheduler for work...
2024-12-22 22:28:54,543 - DEBUG - Done
2024-12-22 22:28:54,544 - DEBUG - There are no more tasks to run at this time
2024-12-22 22:28:54,546 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 22:28:54,546 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 22:28:54,547 - INFO - Worker Worker(salt=5297958816, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=87106) was stopped. Shutting down Keep-Alive thread
2024-12-22 22:28:54,549 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 22:29:25,624 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 22:29:25,628 - ERROR - EXTRACT 'public.products' - FAILED.
2024-12-22 22:29:25,659 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 22:29:25,667 - ERROR - [pid 87248] Worker Worker(salt=4436780551, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=87248) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.products' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 22:29:25,707 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 22:29:25,713 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 22:29:25,714 - DEBUG - Asking scheduler for work...
2024-12-22 22:29:25,717 - DEBUG - Done
2024-12-22 22:29:25,717 - DEBUG - There are no more tasks to run at this time
2024-12-22 22:29:25,718 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 22:29:25,719 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 22:29:25,719 - INFO - Worker Worker(salt=4436780551, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=87248) was stopped. Shutting down Keep-Alive thread
2024-12-22 22:29:25,721 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 23:32:32,261 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 23:32:32,269 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 23:32:32,309 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 23:32:32,322 - ERROR - [pid 102232] Worker Worker(salt=593273724, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=102232) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 23:32:32,383 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 23:32:32,389 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 23:32:32,390 - DEBUG - Asking scheduler for work...
2024-12-22 23:32:32,393 - DEBUG - Done
2024-12-22 23:32:32,394 - DEBUG - There are no more tasks to run at this time
2024-12-22 23:32:32,395 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 23:32:32,395 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 23:32:32,396 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 23:32:32,396 - INFO - Worker Worker(salt=593273724, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=102232) was stopped. Shutting down Keep-Alive thread
2024-12-22 23:32:32,398 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 23:55:26,218 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 23:55:26,223 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 23:55:26,293 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 23:55:26,303 - ERROR - [pid 114574] Worker Worker(salt=743511585, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114574) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 23:55:26,351 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 23:55:26,357 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 23:55:26,358 - DEBUG - Asking scheduler for work...
2024-12-22 23:55:26,360 - DEBUG - Done
2024-12-22 23:55:26,361 - DEBUG - There are no more tasks to run at this time
2024-12-22 23:55:26,361 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 23:55:26,362 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 23:55:26,363 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 23:55:26,363 - INFO - Worker Worker(salt=743511585, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114574) was stopped. Shutting down Keep-Alive thread
2024-12-22 23:55:26,366 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 23:57:46,629 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 23:57:46,633 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-22 23:57:46,683 - INFO - Extract All Tables From Sources - FAILED
2024-12-22 23:57:46,690 - ERROR - [pid 114603] Worker Worker(salt=1478653978, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114603) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-22 23:57:46,737 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 23:57:46,744 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-22 23:57:46,745 - DEBUG - Asking scheduler for work...
2024-12-22 23:57:46,748 - DEBUG - Done
2024-12-22 23:57:46,748 - DEBUG - There are no more tasks to run at this time
2024-12-22 23:57:46,748 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-22 23:57:46,748 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-22 23:57:46,749 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-22 23:57:46,749 - INFO - Worker Worker(salt=1478653978, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114603) was stopped. Shutting down Keep-Alive thread
2024-12-22 23:57:46,750 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 00:11:35,397 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 00:11:35,401 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 00:11:35,478 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 00:11:35,488 - ERROR - [pid 114781] Worker Worker(salt=7395089440, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114781) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5443 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 00:11:35,533 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 00:11:35,539 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 00:11:35,540 - DEBUG - Asking scheduler for work...
2024-12-23 00:11:35,542 - DEBUG - Done
2024-12-23 00:11:35,543 - DEBUG - There are no more tasks to run at this time
2024-12-23 00:11:35,543 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 00:11:35,544 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-23 00:11:35,544 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 00:11:35,544 - INFO - Worker Worker(salt=7395089440, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114781) was stopped. Shutting down Keep-Alive thread
2024-12-23 00:11:35,546 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 00:16:54,447 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 00:16:54,450 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 00:16:54,494 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 00:16:54,500 - ERROR - [pid 114808] Worker Worker(salt=9939307206, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114808) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 00:16:54,537 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 00:16:54,544 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 00:16:54,545 - DEBUG - Asking scheduler for work...
2024-12-23 00:16:54,548 - DEBUG - Done
2024-12-23 00:16:54,548 - DEBUG - There are no more tasks to run at this time
2024-12-23 00:16:54,549 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 00:16:54,549 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-23 00:16:54,549 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 00:16:54,550 - INFO - Worker Worker(salt=9939307206, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114808) was stopped. Shutting down Keep-Alive thread
2024-12-23 00:16:54,551 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 00:19:54,708 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 00:19:54,710 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 00:19:54,753 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 00:19:54,762 - ERROR - [pid 114917] Worker Worker(salt=9277330010, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114917) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 00:19:54,805 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 00:19:54,810 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 00:19:54,811 - DEBUG - Asking scheduler for work...
2024-12-23 00:19:54,813 - DEBUG - Done
2024-12-23 00:19:54,813 - DEBUG - There are no more tasks to run at this time
2024-12-23 00:19:54,814 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 00:19:54,814 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-23 00:19:54,814 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 00:19:54,815 - INFO - Worker Worker(salt=9277330010, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=114917) was stopped. Shutting down Keep-Alive thread
2024-12-23 00:19:54,816 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 01:44:22,767 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 01:44:22,798 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 01:44:22,874 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 01:44:22,892 - ERROR - [pid 472] Worker Worker(salt=989016694, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=472) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5440 failed: FATAL:  password authentication failed for user "postgres"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5440 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 01:44:22,981 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 01:44:22,989 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 01:44:22,990 - DEBUG - Asking scheduler for work...
2024-12-23 01:44:22,993 - DEBUG - Done
2024-12-23 01:44:22,994 - DEBUG - There are no more tasks to run at this time
2024-12-23 01:44:22,994 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 01:44:22,995 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-23 01:44:22,996 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 01:44:22,997 - INFO - Worker Worker(salt=989016694, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=472) was stopped. Shutting down Keep-Alive thread
2024-12-23 01:44:23,001 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 01:45:09,651 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 01:45:09,674 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 01:45:09,782 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 01:45:09,798 - ERROR - [pid 497] Worker Worker(salt=9835193611, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=497) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5440 failed: FATAL:  database "olist" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/user/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5440 failed: FATAL:  database "olist" does not exist

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 01:45:09,877 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 01:45:09,883 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 01:45:09,884 - DEBUG - Asking scheduler for work...
2024-12-23 01:45:09,888 - DEBUG - Done
2024-12-23 01:45:09,888 - DEBUG - There are no more tasks to run at this time
2024-12-23 01:45:09,889 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 01:45:09,890 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 01:45:09,891 - INFO - Worker Worker(salt=9835193611, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=497) was stopped. Shutting down Keep-Alive thread
2024-12-23 01:45:09,892 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 01:45:46,211 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 01:45:46,269 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 01:45:46,318 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 01:45:46,328 - ERROR - [pid 516] Worker Worker(salt=874655761, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=516) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist
LINE 1: SELECT * FROM public.customers;
                      ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 526, in read_sql_query
    return pandas_sql.read_query(
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1836, in read_query
    result = self.execute(sql, params)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1659, in execute
    return self.con.exec_driver_sql(sql, *args)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1782, in exec_driver_sql
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist
LINE 1: SELECT * FROM public.customers;
                      ^

[SQL: SELECT * FROM public.customers;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 01:45:46,380 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 01:45:46,386 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 01:45:46,387 - DEBUG - Asking scheduler for work...
2024-12-23 01:45:46,391 - DEBUG - Done
2024-12-23 01:45:46,392 - DEBUG - There are no more tasks to run at this time
2024-12-23 01:45:46,393 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 01:45:46,393 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 01:45:46,394 - INFO - Worker Worker(salt=874655761, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=516) was stopped. Shutting down Keep-Alive thread
2024-12-23 01:45:46,396 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-23 02:59:27,881 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-23 02:59:27,931 - ERROR - EXTRACT 'public.customers' - FAILED.
2024-12-23 02:59:27,987 - INFO - Extract All Tables From Sources - FAILED
2024-12-23 02:59:27,997 - ERROR - [pid 787] Worker Worker(salt=2783360370, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=787) failed    Extract()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist
LINE 1: SELECT * FROM public.customers;
                      ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 526, in read_sql_query
    return pandas_sql.read_query(
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1836, in read_query
    result = self.execute(sql, params)
  File "/home/user/.local/lib/python3.10/site-packages/pandas/io/sql.py", line 1659, in execute
    return self.con.exec_driver_sql(sql, *args)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1782, in exec_driver_sql
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist
LINE 1: SELECT * FROM public.customers;
                      ^

[SQL: SELECT * FROM public.customers;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.customers' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-23 02:59:28,045 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-23 02:59:28,051 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-23 02:59:28,052 - DEBUG - Asking scheduler for work...
2024-12-23 02:59:28,054 - DEBUG - Done
2024-12-23 02:59:28,054 - DEBUG - There are no more tasks to run at this time
2024-12-23 02:59:28,054 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-23 02:59:28,055 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-23 02:59:28,055 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-23 02:59:28,055 - INFO - Worker Worker(salt=2783360370, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=787) was stopped. Shutting down Keep-Alive thread
2024-12-23 02:59:28,056 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 02:51:06,727 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 02:51:06,861 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 02:51:07,358 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 02:51:07,386 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 02:51:08,287 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 02:51:08,752 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 02:51:09,419 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 02:51:10,222 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 02:51:10,248 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 02:51:10,508 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 02:51:10,509 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 02:51:10,514 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 02:51:10,515 - INFO - [pid 1325] Worker Worker(salt=6183284721, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1325) done      Extract()
2025-01-10 02:51:10,516 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:51:10,519 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 02:51:10,520 - DEBUG - Asking scheduler for work...
2025-01-10 02:51:10,523 - DEBUG - Pending tasks: 2
2025-01-10 02:51:10,523 - INFO - [pid 1325] Worker Worker(salt=6183284721, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1325) running   Load()
2025-01-10 02:51:10,543 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE ;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 02:51:11,777 - INFO - Read Extracted Data - SUCCESS
2025-01-10 02:51:11,779 - INFO - Connect to DWH - SUCCESS
2025-01-10 02:51:11,811 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 02:51:11,813 - ERROR - [pid 1325] Worker Worker(salt=6183284721, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1325) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 144, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 02:51:11,836 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:51:11,841 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 02:51:11,842 - DEBUG - Asking scheduler for work...
2025-01-10 02:51:11,844 - DEBUG - Done
2025-01-10 02:51:11,844 - DEBUG - There are no more tasks to run at this time
2025-01-10 02:51:11,845 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 02:51:11,846 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 02:51:11,846 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 02:51:11,847 - INFO - Worker Worker(salt=6183284721, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1325) was stopped. Shutting down Keep-Alive thread
2025-01-10 02:51:11,848 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 02:52:45,035 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 02:52:45,158 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 02:52:45,682 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 02:52:45,708 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 02:52:46,581 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 02:52:46,994 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 02:52:47,722 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 02:52:48,631 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 02:52:48,660 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 02:52:48,923 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 02:52:48,924 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 02:52:48,930 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 02:52:48,931 - INFO - [pid 1346] Worker Worker(salt=3119685047, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1346) done      Extract()
2025-01-10 02:52:48,933 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:52:48,936 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 02:52:48,937 - DEBUG - Asking scheduler for work...
2025-01-10 02:52:48,940 - DEBUG - Pending tasks: 2
2025-01-10 02:52:48,940 - INFO - [pid 1346] Worker Worker(salt=3119685047, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1346) running   Load()
2025-01-10 02:52:48,966 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 02:52:50,126 - INFO - Read Extracted Data - SUCCESS
2025-01-10 02:52:50,128 - INFO - Connect to DWH - SUCCESS
2025-01-10 02:52:50,143 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 02:52:50,145 - ERROR - [pid 1346] Worker Worker(salt=3119685047, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1346) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 144, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 02:52:50,169 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:52:50,174 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 02:52:50,174 - DEBUG - Asking scheduler for work...
2025-01-10 02:52:50,176 - DEBUG - Done
2025-01-10 02:52:50,177 - DEBUG - There are no more tasks to run at this time
2025-01-10 02:52:50,177 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 02:52:50,178 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 02:52:50,178 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 02:52:50,179 - INFO - Worker Worker(salt=3119685047, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1346) was stopped. Shutting down Keep-Alive thread
2025-01-10 02:52:50,180 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 02:54:48,527 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 02:54:48,656 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 02:54:49,243 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 02:54:49,273 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 02:54:50,259 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 02:54:50,691 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 02:54:51,338 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 02:54:52,221 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 02:54:52,252 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 02:54:52,495 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 02:54:52,496 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 02:54:52,502 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 02:54:52,503 - INFO - [pid 1367] Worker Worker(salt=2103284914, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1367) done      Extract()
2025-01-10 02:54:52,504 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:54:52,508 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 02:54:52,508 - DEBUG - Asking scheduler for work...
2025-01-10 02:54:52,511 - DEBUG - Pending tasks: 2
2025-01-10 02:54:52,511 - INFO - [pid 1367] Worker Worker(salt=2103284914, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1367) running   Load()
2025-01-10 02:54:52,526 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 02:54:53,706 - INFO - Read Extracted Data - SUCCESS
2025-01-10 02:54:53,707 - INFO - Connect to DWH - SUCCESS
2025-01-10 02:54:53,719 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 02:54:53,721 - ERROR - [pid 1367] Worker Worker(salt=2103284914, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1367) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 145, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 02:54:53,744 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 02:54:53,749 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 02:54:53,750 - DEBUG - Asking scheduler for work...
2025-01-10 02:54:53,753 - DEBUG - Done
2025-01-10 02:54:53,754 - DEBUG - There are no more tasks to run at this time
2025-01-10 02:54:53,754 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 02:54:53,755 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 02:54:53,755 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 02:54:53,755 - INFO - Worker Worker(salt=2103284914, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1367) was stopped. Shutting down Keep-Alive thread
2025-01-10 02:54:53,756 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 03:01:09,138 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 03:01:09,239 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 03:01:09,763 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 03:01:09,793 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 03:01:10,659 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 03:01:11,223 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 03:01:11,939 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 03:01:12,944 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 03:01:12,974 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 03:01:13,240 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 03:01:13,242 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 03:01:13,248 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 03:01:13,250 - INFO - [pid 1400] Worker Worker(salt=7550346014, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1400) done      Extract()
2025-01-10 03:01:13,251 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:01:13,256 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 03:01:13,257 - DEBUG - Asking scheduler for work...
2025-01-10 03:01:13,259 - DEBUG - Pending tasks: 2
2025-01-10 03:01:13,260 - INFO - [pid 1400] Worker Worker(salt=7550346014, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1400) running   Load()
2025-01-10 03:01:13,278 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 03:01:14,463 - INFO - Read Extracted Data - SUCCESS
2025-01-10 03:01:14,465 - INFO - Connect to DWH - SUCCESS
2025-01-10 03:01:14,477 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 03:01:14,479 - ERROR - [pid 1400] Worker Worker(salt=7550346014, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1400) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 144, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 03:01:14,504 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:01:14,509 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 03:01:14,510 - DEBUG - Asking scheduler for work...
2025-01-10 03:01:14,512 - DEBUG - Done
2025-01-10 03:01:14,512 - DEBUG - There are no more tasks to run at this time
2025-01-10 03:01:14,513 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 03:01:14,513 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 03:01:14,513 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 03:01:14,514 - INFO - Worker Worker(salt=7550346014, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1400) was stopped. Shutting down Keep-Alive thread
2025-01-10 03:01:14,515 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 03:02:34,897 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 03:02:35,010 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 03:02:35,573 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 03:02:35,600 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 03:02:36,545 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 03:02:36,995 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 03:02:37,685 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 03:02:38,591 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 03:02:38,625 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 03:02:38,888 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 03:02:38,889 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 03:02:38,895 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 03:02:38,896 - INFO - [pid 1421] Worker Worker(salt=4499232462, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1421) done      Extract()
2025-01-10 03:02:38,898 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:02:38,902 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 03:02:38,902 - DEBUG - Asking scheduler for work...
2025-01-10 03:02:38,905 - DEBUG - Pending tasks: 2
2025-01-10 03:02:38,906 - INFO - [pid 1421] Worker Worker(salt=4499232462, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1421) running   Load()
2025-01-10 03:02:38,921 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 03:02:40,017 - INFO - Read Extracted Data - SUCCESS
2025-01-10 03:02:40,018 - INFO - Connect to DWH - SUCCESS
2025-01-10 03:02:40,030 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 03:02:40,032 - ERROR - [pid 1421] Worker Worker(salt=4499232462, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1421) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 144, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 03:02:40,053 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:02:40,057 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 03:02:40,057 - DEBUG - Asking scheduler for work...
2025-01-10 03:02:40,059 - DEBUG - Done
2025-01-10 03:02:40,060 - DEBUG - There are no more tasks to run at this time
2025-01-10 03:02:40,060 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 03:02:40,060 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 03:02:40,061 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 03:02:40,061 - INFO - Worker Worker(salt=4499232462, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1421) was stopped. Shutting down Keep-Alive thread
2025-01-10 03:02:40,063 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 03:09:35,020 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 03:09:35,132 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 03:09:35,674 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 03:09:35,710 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 03:09:36,651 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 03:09:37,098 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 03:09:37,827 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 03:09:38,833 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 03:09:38,865 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 03:09:39,111 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 03:09:39,113 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 03:09:39,120 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 03:09:39,121 - INFO - [pid 1451] Worker Worker(salt=222051670, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1451) done      Extract()
2025-01-10 03:09:39,123 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:09:39,126 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 03:09:39,126 - DEBUG - Asking scheduler for work...
2025-01-10 03:09:39,130 - DEBUG - Pending tasks: 2
2025-01-10 03:09:39,131 - INFO - [pid 1451] Worker Worker(salt=222051670, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1451) running   Load()
2025-01-10 03:09:39,152 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 03:09:40,357 - INFO - Read Extracted Data - SUCCESS
2025-01-10 03:09:40,358 - INFO - Connect to DWH - SUCCESS
2025-01-10 03:09:40,371 - ERROR - Truncate sources Schema in DWH - FAILED
2025-01-10 03:09:40,373 - ERROR - [pid 1451] Worker Worker(salt=222051670, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1451) failed    Load()
Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 133, in run
    session.execute(query)
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/user/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/user/.local/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 144, in run
    raise Exception("Failed to Truncate sources Schema in DWH")
Exception: Failed to Truncate sources Schema in DWH
2025-01-10 03:09:40,398 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 03:09:40,402 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 03:09:40,402 - DEBUG - Asking scheduler for work...
2025-01-10 03:09:40,405 - DEBUG - Done
2025-01-10 03:09:40,405 - DEBUG - There are no more tasks to run at this time
2025-01-10 03:09:40,406 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 03:09:40,406 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 03:09:40,406 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 03:09:40,407 - INFO - Worker Worker(salt=222051670, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1451) was stopped. Shutting down Keep-Alive thread
2025-01-10 03:09:40,409 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 11:17:13,116 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 11:17:13,471 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 11:17:14,395 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 11:17:14,438 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 11:17:16,033 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 11:17:16,881 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 11:17:18,245 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 11:17:19,623 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 11:17:19,653 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 11:17:20,056 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 11:17:20,057 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 11:17:20,089 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 11:17:20,091 - INFO - [pid 1720] Worker Worker(salt=8882184260, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1720) done      Extract()
2025-01-10 11:17:20,092 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 11:17:20,096 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 11:17:20,098 - DEBUG - Asking scheduler for work...
2025-01-10 11:17:20,101 - DEBUG - Pending tasks: 2
2025-01-10 11:17:20,102 - INFO - [pid 1720] Worker Worker(salt=8882184260, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1720) running   Load()
2025-01-10 11:17:20,153 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 11:17:22,340 - INFO - Read Extracted Data - SUCCESS
2025-01-10 11:17:22,342 - INFO - Connect to DWH - SUCCESS
2025-01-10 11:17:22,407 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-10 11:17:22,408 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-10 11:17:22,807 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-10 11:17:22,823 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-10 11:17:24,955 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-10 11:17:25,006 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-10 11:17:26,200 - INFO - LOAD 'public.products' - SUCCESS
2025-01-10 11:17:28,901 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-10 11:17:31,953 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-10 11:17:34,020 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-10 11:17:38,067 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-10 11:17:38,068 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-10 11:17:38,190 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-10 11:17:38,205 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-10 11:17:38,213 - ERROR - [pid 1720] Worker Worker(salt=8882184260, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1720) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column customers.updated_at does not exist
LINE 28:                         stg.customers.updated_at
                                 ^
HINT:  Perhaps you meant to reference the column "customers.created_at".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column customers.updated_at does not exist
LINE 28:                         stg.customers.updated_at
                                 ^
HINT:  Perhaps you meant to reference the column "customers.created_at".

[SQL: INSERT INTO stg.customers 
    (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state)

SELECT
    customer_id,
    customer_unique_id,
    customer_zip_code_prefix,
    customer_city,
    customer_state

FROM public.customers

ON CONFLICT(customer_id) 
DO UPDATE SET
    customer_unique_id = EXCLUDED.customer_unique_id,
    customer_zip_code_prefix = EXCLUDED.customer_zip_code_prefix,
    customer_city = EXCLUDED.customer_city,
    customer_state = EXCLUDED.customer_state,

    updated_at = CASE WHEN 
                        stg.customers.customer_unique_id <> EXCLUDED.customer_unique_id 
                        OR stg.customers.customer_zip_code_prefix <> EXCLUDED.customer_zip_code_prefix 
                        OR stg.customers.customer_city <> EXCLUDED.customer_city 
                        OR stg.customers.customer_state <> EXCLUDED.customer_state 
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.customers.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-10 11:17:38,506 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 11:17:38,512 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 11:17:38,513 - DEBUG - Asking scheduler for work...
2025-01-10 11:17:38,515 - DEBUG - Done
2025-01-10 11:17:38,516 - DEBUG - There are no more tasks to run at this time
2025-01-10 11:17:38,516 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 11:17:38,517 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 11:17:38,517 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 11:17:38,518 - INFO - Worker Worker(salt=8882184260, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1720) was stopped. Shutting down Keep-Alive thread
2025-01-10 11:17:38,519 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 12:38:36,917 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 12:38:37,153 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 12:38:38,061 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 12:38:38,103 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 12:38:39,666 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 12:38:40,268 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 12:38:41,657 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 12:38:43,267 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 12:38:43,299 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 12:38:43,666 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 12:38:43,667 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 12:38:43,697 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 12:38:43,698 - INFO - [pid 1954] Worker Worker(salt=1031136554, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1954) done      Extract()
2025-01-10 12:38:43,700 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 12:38:43,704 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 12:38:43,705 - DEBUG - Asking scheduler for work...
2025-01-10 12:38:43,708 - DEBUG - Pending tasks: 2
2025-01-10 12:38:43,709 - INFO - [pid 1954] Worker Worker(salt=1031136554, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1954) running   Load()
2025-01-10 12:38:43,744 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 12:38:45,867 - INFO - Read Extracted Data - SUCCESS
2025-01-10 12:38:45,868 - INFO - Connect to DWH - SUCCESS
2025-01-10 12:38:45,959 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-10 12:38:45,960 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-10 12:38:46,354 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-10 12:38:46,364 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-10 12:38:48,171 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-10 12:38:48,218 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-10 12:38:49,400 - INFO - LOAD 'public.products' - SUCCESS
2025-01-10 12:38:51,922 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-10 12:38:54,669 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-10 12:38:56,555 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-10 12:38:58,715 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-10 12:38:58,716 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-10 12:38:59,710 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-10 12:38:59,723 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-10 12:38:59,729 - ERROR - [pid 1954] Worker Worker(salt=1031136554, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1954) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column customers.updated_at does not exist
LINE 28:                         stg.customers.updated_at
                                 ^
HINT:  Perhaps you meant to reference the column "customers.created_at".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column customers.updated_at does not exist
LINE 28:                         stg.customers.updated_at
                                 ^
HINT:  Perhaps you meant to reference the column "customers.created_at".

[SQL: INSERT INTO stg.customers 
    (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state)

SELECT
    customer_id,
    customer_unique_id,
    customer_zip_code_prefix,
    customer_city,
    customer_state

FROM public.customers

ON CONFLICT(customer_id) 
DO UPDATE SET
    customer_unique_id = EXCLUDED.customer_unique_id,
    customer_zip_code_prefix = EXCLUDED.customer_zip_code_prefix,
    customer_city = EXCLUDED.customer_city,
    customer_state = EXCLUDED.customer_state,

    updated_at = CASE WHEN 
                        stg.customers.customer_unique_id <> EXCLUDED.customer_unique_id 
                        OR stg.customers.customer_zip_code_prefix <> EXCLUDED.customer_zip_code_prefix 
                        OR stg.customers.customer_city <> EXCLUDED.customer_city 
                        OR stg.customers.customer_state <> EXCLUDED.customer_state 
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.customers.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-10 12:38:59,853 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 12:38:59,858 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-10 12:38:59,858 - DEBUG - Asking scheduler for work...
2025-01-10 12:38:59,861 - DEBUG - Done
2025-01-10 12:38:59,861 - DEBUG - There are no more tasks to run at this time
2025-01-10 12:38:59,861 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-10 12:38:59,861 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-10 12:38:59,862 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-10 12:38:59,862 - INFO - Worker Worker(salt=1031136554, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1954) was stopped. Shutting down Keep-Alive thread
2025-01-10 12:38:59,864 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-10 12:47:36,483 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-10 12:47:36,831 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-10 12:47:37,731 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-10 12:47:37,771 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-10 12:47:39,190 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-10 12:47:39,883 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-10 12:47:40,860 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-10 12:47:42,276 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-10 12:47:42,311 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-10 12:47:42,678 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-10 12:47:42,679 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-10 12:47:42,698 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-10 12:47:42,700 - INFO - [pid 1991] Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) done      Extract()
2025-01-10 12:47:42,701 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 12:47:42,704 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-10 12:47:42,705 - DEBUG - Asking scheduler for work...
2025-01-10 12:47:42,708 - DEBUG - Pending tasks: 2
2025-01-10 12:47:42,709 - INFO - [pid 1991] Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) running   Load()
2025-01-10 12:47:42,747 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-10 12:47:44,706 - INFO - Read Extracted Data - SUCCESS
2025-01-10 12:47:44,707 - INFO - Connect to DWH - SUCCESS
2025-01-10 12:47:44,794 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-10 12:47:44,795 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-10 12:47:45,180 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-10 12:47:45,190 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-10 12:47:47,035 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-10 12:47:47,083 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-10 12:47:48,231 - INFO - LOAD 'public.products' - SUCCESS
2025-01-10 12:47:50,819 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-10 12:47:53,575 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-10 12:47:55,474 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-10 12:47:57,624 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-10 12:47:57,624 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-10 12:48:06,013 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-10 12:48:06,024 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-10 12:48:06,049 - INFO - [pid 1991] Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) done      Load()
2025-01-10 12:48:06,050 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 12:48:06,054 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-10 12:48:06,054 - DEBUG - Asking scheduler for work...
2025-01-10 12:48:06,057 - DEBUG - Pending tasks: 1
2025-01-10 12:48:06,057 - INFO - [pid 1991] Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) running   Transform()
2025-01-10 12:48:06,231 - INFO - Read Transform Query - SUCCESS
2025-01-10 12:48:06,232 - INFO - Connect to DWH - SUCCESS
2025-01-10 12:48:06,233 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-10 12:48:06,278 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-10 12:48:06,292 - ERROR - Transform Tables - FAILED
2025-01-10 12:48:06,297 - ERROR - [pid 1991] Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "latitude" does not exist
LINE 7:         latitude,
                ^
DETAIL:  There is a column named "latitude" in table "final", but it cannot be referenced from this part of the query.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 113, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "latitude" does not exist
LINE 7:         latitude,
                ^
DETAIL:  There is a column named "latitude" in table "final", but it cannot be referenced from this part of the query.

[SQL: MERGE INTO final.dim_customers AS final
USING (
    SELECT 
        customer_id AS customer_nk,
        customer_unique_id,
        customer_zip_code_prefix,
        latitude,
        longitude,
        customer_city,
        customer_state,
        CURRENT_TIMESTAMP AS created_at
    FROM stg.customers
) AS staging

ON final.customer_nk = staging.customer_nk

WHEN MATCHED AND (
    final.customer_zip_code_prefix <> staging.customer_zip_code_prefix OR
    final.customer_city <> staging.customer_city OR
    final.customer_state <> staging.customer_state
) THEN
    UPDATE SET 
        current_flag = 'Expired',
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        customer_id, 
        customer_nk, 
        customer_unique_id, 
        customer_zip_code_prefix, 
        latitude, 
        longitude,        
        customer_city, 
        customer_state, 
        created_at, 
        updated_at, 
        current_flag
    )
    VALUES (
        gen_random_uuid(),
        staging.customer_nk, 
        staging.customer_unique_id, 
        staging.customer_zip_code_prefix, 
        staging.latitude, 
        staging.longitude,
        staging.customer_city, 
        staging.customer_state, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP, 
        'Current'
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-10 12:48:06,536 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-10 12:48:06,544 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-10 12:48:06,544 - DEBUG - Asking scheduler for work...
2025-01-10 12:48:06,551 - DEBUG - Done
2025-01-10 12:48:06,551 - DEBUG - There are no more tasks to run at this time
2025-01-10 12:48:06,552 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-10 12:48:06,552 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-10 12:48:06,553 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-10 12:48:06,553 - INFO - Worker Worker(salt=7351372528, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=1991) was stopped. Shutting down Keep-Alive thread
2025-01-10 12:48:06,554 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-12 10:57:18,961 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-12 10:57:19,478 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-12 10:57:20,494 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-12 10:57:20,539 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-12 10:57:22,389 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-12 10:57:23,398 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-12 10:57:24,734 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-12 10:57:26,234 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-12 10:57:26,271 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-12 10:57:26,686 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-12 10:57:26,688 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-12 10:57:26,711 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-12 10:57:26,712 - INFO - [pid 4793] Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) done      Extract()
2025-01-12 10:57:26,714 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-12 10:57:26,717 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-12 10:57:26,718 - DEBUG - Asking scheduler for work...
2025-01-12 10:57:26,721 - DEBUG - Pending tasks: 2
2025-01-12 10:57:26,721 - INFO - [pid 4793] Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) running   Load()
2025-01-12 10:57:26,765 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-12 10:57:29,528 - INFO - Read Extracted Data - SUCCESS
2025-01-12 10:57:29,529 - INFO - Connect to DWH - SUCCESS
2025-01-12 10:57:29,660 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-12 10:57:29,661 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-12 10:57:30,196 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-12 10:57:30,211 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-12 10:57:32,789 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-12 10:57:32,855 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-12 10:57:34,212 - INFO - LOAD 'public.products' - SUCCESS
2025-01-12 10:57:37,534 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-12 10:57:41,052 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-12 10:57:43,662 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-12 10:57:46,306 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-12 10:57:46,307 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-12 10:57:58,957 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-12 10:57:58,966 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-12 10:57:58,993 - INFO - [pid 4793] Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) done      Load()
2025-01-12 10:57:58,994 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-12 10:57:58,997 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-12 10:57:58,998 - DEBUG - Asking scheduler for work...
2025-01-12 10:57:59,002 - DEBUG - Pending tasks: 1
2025-01-12 10:57:59,003 - INFO - [pid 4793] Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) running   Transform()
2025-01-12 10:57:59,166 - INFO - Read Transform Query - SUCCESS
2025-01-12 10:57:59,167 - INFO - Connect to DWH - SUCCESS
2025-01-12 10:57:59,167 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-12 10:57:59,192 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-12 10:57:59,203 - ERROR - Transform Tables - FAILED
2025-01-12 10:57:59,209 - ERROR - [pid 4793] Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "latitude" does not exist
LINE 7:         latitude,
                ^
DETAIL:  There is a column named "latitude" in table "final", but it cannot be referenced from this part of the query.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 113, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "latitude" does not exist
LINE 7:         latitude,
                ^
DETAIL:  There is a column named "latitude" in table "final", but it cannot be referenced from this part of the query.

[SQL: MERGE INTO final.dim_customers AS final
USING (
    SELECT 
        customer_id AS customer_nk,
        customer_unique_id,
        customer_zip_code_prefix,
        latitude,
        longitude,
        customer_city,
        customer_state,
        CURRENT_TIMESTAMP AS created_at
    FROM stg.customers
) AS staging

ON final.customer_nk = staging.customer_nk

WHEN MATCHED AND (
    final.customer_zip_code_prefix <> staging.customer_zip_code_prefix OR
    final.customer_city <> staging.customer_city OR
    final.customer_state <> staging.customer_state
) THEN
    UPDATE SET 
        current_flag = 'Expired',
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        customer_id, 
        customer_nk, 
        customer_unique_id, 
        customer_zip_code_prefix, 
        latitude, 
        longitude,        
        customer_city, 
        customer_state, 
        created_at, 
        updated_at, 
        current_flag
    )
    VALUES (
        gen_random_uuid(),
        staging.customer_nk, 
        staging.customer_unique_id, 
        staging.customer_zip_code_prefix, 
        staging.latitude, 
        staging.longitude,
        staging.customer_city, 
        staging.customer_state, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP, 
        'Current'
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-12 10:57:59,546 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-12 10:57:59,557 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-12 10:57:59,557 - DEBUG - Asking scheduler for work...
2025-01-12 10:57:59,560 - DEBUG - Done
2025-01-12 10:57:59,560 - DEBUG - There are no more tasks to run at this time
2025-01-12 10:57:59,560 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-12 10:57:59,561 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-12 10:57:59,561 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-12 10:57:59,561 - INFO - Worker Worker(salt=1894085401, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=4793) was stopped. Shutting down Keep-Alive thread
2025-01-12 10:57:59,563 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 09:04:19,895 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 09:04:20,400 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 09:04:21,288 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 09:04:21,336 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 09:04:22,964 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 09:04:23,783 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 09:04:25,165 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 09:04:27,030 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 09:04:27,073 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 09:04:27,575 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 09:04:27,577 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 09:04:27,612 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 09:04:27,614 - INFO - [pid 6730] Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) done      Extract()
2025-01-13 09:04:27,616 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:04:27,621 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 09:04:27,622 - DEBUG - Asking scheduler for work...
2025-01-13 09:04:27,626 - DEBUG - Pending tasks: 2
2025-01-13 09:04:27,627 - INFO - [pid 6730] Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) running   Load()
2025-01-13 09:04:27,679 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 09:04:30,286 - INFO - Read Extracted Data - SUCCESS
2025-01-13 09:04:30,287 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:04:30,415 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 09:04:30,416 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 09:04:30,932 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 09:04:30,944 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 09:04:33,532 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 09:04:33,597 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 09:04:35,067 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 09:04:38,355 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 09:04:41,698 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 09:04:44,106 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 09:04:46,899 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 09:04:46,899 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 09:04:58,377 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 09:04:58,386 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 09:04:58,412 - INFO - [pid 6730] Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) done      Load()
2025-01-13 09:04:58,413 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:04:58,417 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 09:04:58,418 - DEBUG - Asking scheduler for work...
2025-01-13 09:04:58,421 - DEBUG - Pending tasks: 1
2025-01-13 09:04:58,422 - INFO - [pid 6730] Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) running   Transform()
2025-01-13 09:04:58,600 - INFO - Read Transform Query - SUCCESS
2025-01-13 09:04:58,601 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:04:58,602 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 09:04:58,634 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 09:04:58,647 - ERROR - Transform Tables - FAILED
2025-01-13 09:04:58,654 - ERROR - [pid 6730] Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 113, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: MERGE INTO final.dim_customers AS final
USING (
    SELECT 
        customer_id AS customer_nk,
        customer_unique_id,
        customer_zip_code_prefix,
        -- latitude,
        -- longitude,
        customer_city,
        customer_state,
        CURRENT_TIMESTAMP AS created_at
    FROM stg.customers
) AS staging

ON final.customer_nk = staging.customer_nk

WHEN MATCHED AND (
    final.customer_zip_code_prefix <> staging.customer_zip_code_prefix OR
    final.customer_city <> staging.customer_city OR
    final.customer_state <> staging.customer_state
) THEN
    UPDATE SET 
        current_flag = 'Expired',
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        customer_id, 
        customer_nk, 
        customer_unique_id, 
        customer_zip_code_prefix, 
        -- latitude, 
        -- longitude,        
        customer_city, 
        customer_state, 
        created_at, 
        updated_at, 
        current_flag
    )
    VALUES (
        gen_random_uuid(),
        staging.customer_nk, 
        staging.customer_unique_id, 
        staging.customer_zip_code_prefix, 
        -- staging.latitude, 
        -- staging.longitude,
        staging.customer_city, 
        staging.customer_state, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP, 
        'Current'
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 09:04:58,936 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:04:58,944 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 09:04:58,944 - DEBUG - Asking scheduler for work...
2025-01-13 09:04:58,947 - DEBUG - Done
2025-01-13 09:04:58,947 - DEBUG - There are no more tasks to run at this time
2025-01-13 09:04:58,948 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 09:04:58,948 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-13 09:04:58,948 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 09:04:58,949 - INFO - Worker Worker(salt=585194289, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6730) was stopped. Shutting down Keep-Alive thread
2025-01-13 09:04:58,950 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 09:06:08,408 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 09:06:08,672 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 09:06:09,684 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 09:06:09,724 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 09:06:11,238 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 09:06:11,990 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 09:06:13,326 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 09:06:15,009 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 09:06:15,049 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 09:06:15,450 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 09:06:15,452 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 09:06:15,474 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 09:06:15,476 - INFO - [pid 6755] Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) done      Extract()
2025-01-13 09:06:15,478 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:06:15,482 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 09:06:15,483 - DEBUG - Asking scheduler for work...
2025-01-13 09:06:15,486 - DEBUG - Pending tasks: 2
2025-01-13 09:06:15,487 - INFO - [pid 6755] Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) running   Load()
2025-01-13 09:06:15,514 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 09:06:17,674 - INFO - Read Extracted Data - SUCCESS
2025-01-13 09:06:17,675 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:06:17,766 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 09:06:17,766 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 09:06:18,173 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 09:06:18,183 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 09:06:20,239 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 09:06:20,347 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 09:06:21,597 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 09:06:24,449 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 09:06:27,549 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 09:06:29,754 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 09:06:32,178 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 09:06:32,179 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 09:06:41,901 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 09:06:41,910 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 09:06:41,935 - INFO - [pid 6755] Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) done      Load()
2025-01-13 09:06:41,936 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:06:41,939 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 09:06:41,940 - DEBUG - Asking scheduler for work...
2025-01-13 09:06:41,944 - DEBUG - Pending tasks: 1
2025-01-13 09:06:41,944 - INFO - [pid 6755] Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) running   Transform()
2025-01-13 09:06:42,031 - INFO - Read Transform Query - SUCCESS
2025-01-13 09:06:42,032 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:06:42,033 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 09:06:42,055 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 09:06:42,067 - ERROR - Transform Tables - FAILED
2025-01-13 09:06:42,072 - ERROR - [pid 6755] Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 113, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: MERGE INTO final.dim_customers AS final
USING (
    SELECT 
        customer_id AS customer_nk,
        customer_unique_id,
        customer_zip_code_prefix,
        -- latitude,
        -- longitude,
        customer_city,
        customer_state,
        CURRENT_TIMESTAMP AS created_at
    FROM stg.customers
) AS staging

ON final.customer_nk = staging.customer_nk

WHEN MATCHED AND (
    final.customer_zip_code_prefix <> staging.customer_zip_code_prefix OR
    final.customer_city <> staging.customer_city OR
    final.customer_state <> staging.customer_state
) THEN
    UPDATE SET 
        current_flag = 'Expired',
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        customer_id, 
        customer_nk, 
        customer_unique_id, 
        customer_zip_code_prefix, 
        -- latitude, 
        -- longitude,        
        customer_city, 
        customer_state, 
        created_at, 
        updated_at, 
        current_flag
    )
    VALUES (
        gen_random_uuid(),
        staging.customer_nk, 
        staging.customer_unique_id, 
        staging.customer_zip_code_prefix, 
        -- staging.latitude, 
        -- staging.longitude,
        staging.customer_city, 
        staging.customer_state, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP, 
        'Current'
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 09:06:42,316 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:06:42,323 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 09:06:42,324 - DEBUG - Asking scheduler for work...
2025-01-13 09:06:42,326 - DEBUG - Done
2025-01-13 09:06:42,326 - DEBUG - There are no more tasks to run at this time
2025-01-13 09:06:42,327 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 09:06:42,327 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-13 09:06:42,328 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 09:06:42,328 - INFO - Worker Worker(salt=9940024117, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6755) was stopped. Shutting down Keep-Alive thread
2025-01-13 09:06:42,329 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 09:07:51,205 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 09:07:51,438 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 09:07:52,226 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 09:07:52,261 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 09:07:53,797 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 09:07:54,486 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 09:07:55,657 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 09:07:57,066 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 09:07:57,106 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 09:07:57,513 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 09:07:57,515 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 09:07:57,544 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 09:07:57,546 - INFO - [pid 6776] Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) done      Extract()
2025-01-13 09:07:57,547 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:07:57,552 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 09:07:57,553 - DEBUG - Asking scheduler for work...
2025-01-13 09:07:57,556 - DEBUG - Pending tasks: 2
2025-01-13 09:07:57,556 - INFO - [pid 6776] Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) running   Load()
2025-01-13 09:07:57,591 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 09:07:59,547 - INFO - Read Extracted Data - SUCCESS
2025-01-13 09:07:59,549 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:07:59,633 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 09:07:59,633 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 09:08:00,009 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 09:08:00,019 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 09:08:01,887 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 09:08:01,937 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 09:08:03,177 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 09:08:05,935 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 09:08:08,892 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 09:08:10,989 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 09:08:13,341 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 09:08:13,343 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 09:08:22,869 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 09:08:22,878 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 09:08:22,904 - INFO - [pid 6776] Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) done      Load()
2025-01-13 09:08:22,905 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:08:22,909 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 09:08:22,909 - DEBUG - Asking scheduler for work...
2025-01-13 09:08:22,912 - DEBUG - Pending tasks: 1
2025-01-13 09:08:22,912 - INFO - [pid 6776] Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) running   Transform()
2025-01-13 09:08:22,947 - INFO - Read Transform Query - SUCCESS
2025-01-13 09:08:22,948 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:08:22,949 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 09:08:22,963 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 09:08:22,974 - ERROR - Transform Tables - FAILED
2025-01-13 09:08:22,979 - ERROR - [pid 6776] Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 113, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: character varying <> integer
LINE 18:     final.customer_zip_code_prefix <> staging.customer_zip_c...
                                            ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: MERGE INTO final.dim_customers AS final
USING (
    SELECT 
        customer_id AS customer_nk,
        customer_unique_id,
        customer_zip_code_prefix,
        -- latitude,
        -- longitude,
        customer_city,
        customer_state,
        CURRENT_TIMESTAMP AS created_at
    FROM stg.customers
) AS staging

ON final.customer_nk = staging.customer_nk

WHEN MATCHED AND (
    final.customer_zip_code_prefix <> staging.customer_zip_code_prefix OR
    final.customer_city <> staging.customer_city OR
    final.customer_state <> staging.customer_state
) THEN
    UPDATE SET 
        current_flag = 'Expired',
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        customer_id, 
        customer_nk, 
        customer_unique_id, 
        customer_zip_code_prefix, 
        -- latitude, 
        -- longitude,        
        customer_city, 
        customer_state, 
        created_at, 
        updated_at, 
        current_flag
    )
    VALUES (
        gen_random_uuid(),
        staging.customer_nk, 
        staging.customer_unique_id, 
        staging.customer_zip_code_prefix, 
        -- staging.latitude, 
        -- staging.longitude,
        staging.customer_city, 
        staging.customer_state, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP, 
        'Current'
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 09:08:23,071 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:08:23,077 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 09:08:23,078 - DEBUG - Asking scheduler for work...
2025-01-13 09:08:23,080 - DEBUG - Done
2025-01-13 09:08:23,081 - DEBUG - There are no more tasks to run at this time
2025-01-13 09:08:23,081 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 09:08:23,081 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 09:08:23,083 - INFO - Worker Worker(salt=2005520220, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6776) was stopped. Shutting down Keep-Alive thread
2025-01-13 09:08:23,084 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 09:11:04,497 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 09:11:04,779 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 09:11:05,790 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 09:11:05,829 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 09:11:07,277 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 09:11:07,969 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 09:11:09,111 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 09:11:10,752 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 09:11:10,787 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 09:11:11,182 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 09:11:11,184 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 09:11:11,201 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 09:11:11,203 - INFO - [pid 6798] Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) done      Extract()
2025-01-13 09:11:11,204 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:11:11,207 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 09:11:11,208 - DEBUG - Asking scheduler for work...
2025-01-13 09:11:11,212 - DEBUG - Pending tasks: 2
2025-01-13 09:11:11,213 - INFO - [pid 6798] Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) running   Load()
2025-01-13 09:11:11,250 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 09:11:13,380 - INFO - Read Extracted Data - SUCCESS
2025-01-13 09:11:13,381 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:11:13,466 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 09:11:13,467 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 09:11:13,851 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 09:11:13,861 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 09:11:15,825 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 09:11:15,881 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 09:11:17,096 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 09:11:20,274 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 09:11:23,092 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 09:11:25,173 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 09:11:27,496 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 09:11:27,497 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 09:11:36,102 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 09:11:36,108 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 09:11:36,135 - INFO - [pid 6798] Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) done      Load()
2025-01-13 09:11:36,136 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:11:36,139 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 09:11:36,140 - DEBUG - Asking scheduler for work...
2025-01-13 09:11:36,142 - DEBUG - Pending tasks: 1
2025-01-13 09:11:36,143 - INFO - [pid 6798] Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) running   Transform()
2025-01-13 09:11:36,214 - INFO - Read Transform Query - SUCCESS
2025-01-13 09:11:36,216 - INFO - Connect to DWH - SUCCESS
2025-01-13 09:11:36,217 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 09:11:37,094 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-13 09:11:37,095 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 09:11:37,107 - ERROR - Transform Tables - FAILED
2025-01-13 09:11:37,112 - ERROR - [pid 6798] Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 117, in run
    query = sqlalchemy.text(dim_order_items_query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/_elements_constructors.py", line 1640, in text
    return TextClause(text)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 2295, in __init__
    self.text = self._bind_params_regex.sub(repl, text)
TypeError: expected string or bytes-like object

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 09:11:37,254 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 09:11:37,261 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 09:11:37,262 - DEBUG - Asking scheduler for work...
2025-01-13 09:11:37,266 - DEBUG - Done
2025-01-13 09:11:37,266 - DEBUG - There are no more tasks to run at this time
2025-01-13 09:11:37,267 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 09:11:37,267 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-13 09:11:37,268 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 09:11:37,269 - INFO - Worker Worker(salt=9716387925, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=6798) was stopped. Shutting down Keep-Alive thread
2025-01-13 09:11:37,270 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 12:52:58,941 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 12:52:59,287 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 12:52:59,978 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 12:53:00,022 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 12:53:01,106 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 12:53:01,730 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 12:53:03,112 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 12:53:04,424 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 12:53:04,457 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 12:53:04,748 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 12:53:04,750 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 12:53:04,776 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 12:53:04,778 - INFO - [pid 7358] Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) done      Extract()
2025-01-13 12:53:04,779 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:53:04,782 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 12:53:04,782 - DEBUG - Asking scheduler for work...
2025-01-13 12:53:04,786 - DEBUG - Pending tasks: 2
2025-01-13 12:53:04,787 - INFO - [pid 7358] Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) running   Load()
2025-01-13 12:53:04,804 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 12:53:06,344 - INFO - Read Extracted Data - SUCCESS
2025-01-13 12:53:06,346 - INFO - Connect to DWH - SUCCESS
2025-01-13 12:53:06,444 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 12:53:06,444 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 12:53:06,781 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 12:53:06,791 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 12:53:08,521 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 12:53:08,572 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 12:53:09,533 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 12:53:11,776 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 12:53:14,120 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 12:53:15,835 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 12:53:18,009 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 12:53:18,010 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 12:53:26,071 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 12:53:26,082 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 12:53:26,110 - INFO - [pid 7358] Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) done      Load()
2025-01-13 12:53:26,111 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:53:26,114 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 12:53:26,115 - DEBUG - Asking scheduler for work...
2025-01-13 12:53:26,117 - DEBUG - Pending tasks: 1
2025-01-13 12:53:26,118 - INFO - [pid 7358] Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) running   Transform()
2025-01-13 12:53:26,175 - INFO - Read Transform Query - SUCCESS
2025-01-13 12:53:26,177 - INFO - Connect to DWH - SUCCESS
2025-01-13 12:53:26,177 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 12:53:26,864 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-13 12:53:26,870 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 12:53:26,889 - ERROR - Transform Tables - FAILED
2025-01-13 12:53:26,894 - ERROR - [pid 7358] Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "staging.order_items" does not exist
LINE 2: USING staging.order_items AS staging
              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "staging.order_items" does not exist
LINE 2: USING staging.order_items AS staging
              ^

[SQL: MERGE INTO final.dim_order_items AS final
USING staging.order_items AS staging
ON final.order_item_id = staging.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = staging.price,
        freight_value = staging.freight_value,
        order_nk = staging.order_id,
        product_id = staging.product_id,
        seller_nk = staging.seller_id,
        shipping_limit_date = staging.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        staging.order_item_id, 
        staging.price, 
        staging.freight_value, 
        staging.order_id, 
        staging.product_id, 
        staging.seller_id, 
        staging.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 12:53:27,039 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:53:27,046 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 12:53:27,047 - DEBUG - Asking scheduler for work...
2025-01-13 12:53:27,049 - DEBUG - Done
2025-01-13 12:53:27,049 - DEBUG - There are no more tasks to run at this time
2025-01-13 12:53:27,050 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 12:53:27,050 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-13 12:53:27,051 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 12:53:27,051 - INFO - Worker Worker(salt=3475443212, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7358) was stopped. Shutting down Keep-Alive thread
2025-01-13 12:53:27,052 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-13 12:56:16,997 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-13 12:56:17,234 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-13 12:56:17,836 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-13 12:56:17,866 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-13 12:56:18,831 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-13 12:56:19,339 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-13 12:56:20,176 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-13 12:56:21,430 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-13 12:56:21,463 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-13 12:56:21,776 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-13 12:56:21,777 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-13 12:56:21,797 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-13 12:56:21,798 - INFO - [pid 7389] Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) done      Extract()
2025-01-13 12:56:21,802 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:56:21,806 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-13 12:56:21,806 - DEBUG - Asking scheduler for work...
2025-01-13 12:56:21,809 - DEBUG - Pending tasks: 2
2025-01-13 12:56:21,810 - INFO - [pid 7389] Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) running   Load()
2025-01-13 12:56:21,838 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-13 12:56:23,386 - INFO - Read Extracted Data - SUCCESS
2025-01-13 12:56:23,387 - INFO - Connect to DWH - SUCCESS
2025-01-13 12:56:23,465 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-13 12:56:23,466 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-13 12:56:23,770 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-13 12:56:23,779 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-13 12:56:25,281 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-13 12:56:25,330 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-13 12:56:26,229 - INFO - LOAD 'public.products' - SUCCESS
2025-01-13 12:56:28,268 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-13 12:56:30,436 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-13 12:56:31,989 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-13 12:56:33,734 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-13 12:56:33,735 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-13 12:56:41,525 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-13 12:56:41,537 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-13 12:56:41,563 - INFO - [pid 7389] Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) done      Load()
2025-01-13 12:56:41,564 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:56:41,567 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-13 12:56:41,567 - DEBUG - Asking scheduler for work...
2025-01-13 12:56:41,570 - DEBUG - Pending tasks: 1
2025-01-13 12:56:41,570 - INFO - [pid 7389] Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) running   Transform()
2025-01-13 12:56:41,660 - INFO - Read Transform Query - SUCCESS
2025-01-13 12:56:41,662 - INFO - Connect to DWH - SUCCESS
2025-01-13 12:56:41,662 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-13 12:56:42,323 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-13 12:56:42,327 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-13 12:56:42,339 - ERROR - Transform Tables - FAILED
2025-01-13 12:56:42,344 - ERROR - [pid 7389] Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: uuid = integer
LINE 3: ON final.order_item_id = stg.order_item_id
                               ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: uuid = integer
LINE 3: ON final.order_item_id = stg.order_item_id
                               ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-13 12:56:42,471 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-13 12:56:42,477 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-13 12:56:42,477 - DEBUG - Asking scheduler for work...
2025-01-13 12:56:42,479 - DEBUG - Done
2025-01-13 12:56:42,479 - DEBUG - There are no more tasks to run at this time
2025-01-13 12:56:42,480 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-13 12:56:42,480 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-13 12:56:42,480 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-13 12:56:42,481 - INFO - Worker Worker(salt=8110343178, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=7389) was stopped. Shutting down Keep-Alive thread
2025-01-13 12:56:42,481 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 00:57:58,471 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 00:57:58,990 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 00:57:59,874 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 00:57:59,918 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 00:58:01,334 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 00:58:02,028 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 00:58:03,054 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 00:58:05,211 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 00:58:05,252 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 00:58:05,663 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 00:58:05,665 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 00:58:05,701 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 00:58:05,704 - INFO - [pid 9992] Worker Worker(salt=5765037320, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=9992) done      Extract()
2025-01-14 00:58:05,708 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 00:58:05,715 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 00:58:05,716 - DEBUG - Asking scheduler for work...
2025-01-14 00:58:05,722 - DEBUG - Pending tasks: 2
2025-01-14 00:58:05,722 - INFO - [pid 9992] Worker Worker(salt=5765037320, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=9992) running   Load()
2025-01-14 00:58:05,771 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 00:58:08,199 - INFO - Read Extracted Data - SUCCESS
2025-01-14 00:58:08,201 - INFO - Connect to DWH - SUCCESS
2025-01-14 00:58:08,339 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 00:58:08,339 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 00:58:08,754 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 00:58:08,767 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 00:58:10,912 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 00:58:10,962 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 00:58:11,968 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 00:58:14,529 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 00:58:17,128 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 00:58:18,949 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 00:58:21,030 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 00:58:21,031 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 00:58:21,255 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-14 00:58:21,270 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-14 00:58:21,274 - ERROR - [pid 9992] Worker Worker(salt=5765037320, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=9992) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "latitude" of relation "customers" does not exist
LINE 2: ...id, customer_unique_id, customer_zip_code_prefix, latitude, ...
                                                             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "latitude" of relation "customers" does not exist
LINE 2: ...id, customer_unique_id, customer_zip_code_prefix, latitude, ...
                                                             ^

[SQL: INSERT INTO stg.customers 
    (customer_id, customer_unique_id, customer_zip_code_prefix, latitude, longitude, customer_city, customer_state)

SELECT
    c.customer_id,
    c.customer_unique_id,
    c.customer_zip_code_prefix,
    g.geolocation_lat as latitude,
    g.geolocation_lng as longitude,
    c.customer_city,
    c.customer_state

FROM public.customers c

JOIN public.geolocation g
    ON c.customer_zip_code_prefix = g.geolocation_zip_code_prefix

ON CONFLICT(customer_id) 
DO UPDATE SET
    customer_unique_id = EXCLUDED.customer_unique_id,
    customer_zip_code_prefix = EXCLUDED.customer_zip_code_prefix,
    latitude = EXCLUDED.latitude,
    longitude = EXCLUDED.longitude,
    customer_city = EXCLUDED.customer_city,
    customer_state = EXCLUDED.customer_state,

    updated_at = CASE WHEN 
                        stg.customers.customer_unique_id <> EXCLUDED.customer_unique_id 
                        OR stg.customers.customer_zip_code_prefix <> EXCLUDED.customer_zip_code_prefix 
                        OR stg.customers.latitude <> EXCLUDED.latitude 
                        OR stg.customers.longitude <> EXCLUDED.longitude 
                        OR stg.customers.customer_city <> EXCLUDED.customer_city 
                        OR stg.customers.customer_state <> EXCLUDED.customer_state 
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.customers.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-14 00:58:21,794 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 00:58:21,806 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-14 00:58:21,806 - DEBUG - Asking scheduler for work...
2025-01-14 00:58:21,809 - DEBUG - Done
2025-01-14 00:58:21,810 - DEBUG - There are no more tasks to run at this time
2025-01-14 00:58:21,810 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-14 00:58:21,811 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-14 00:58:21,811 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-14 00:58:21,812 - INFO - Worker Worker(salt=5765037320, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=9992) was stopped. Shutting down Keep-Alive thread
2025-01-14 00:58:21,814 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:00:27,490 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:00:27,898 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:00:28,733 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:00:28,774 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:00:30,130 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:00:30,763 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:00:31,842 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:00:33,681 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:00:33,723 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:00:34,169 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:00:34,171 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:00:34,204 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:00:34,206 - INFO - [pid 10013] Worker Worker(salt=8461847662, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10013) done      Extract()
2025-01-14 01:00:34,208 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:00:34,211 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:00:34,212 - DEBUG - Asking scheduler for work...
2025-01-14 01:00:34,215 - DEBUG - Pending tasks: 2
2025-01-14 01:00:34,216 - INFO - [pid 10013] Worker Worker(salt=8461847662, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10013) running   Load()
2025-01-14 01:00:34,244 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:00:35,993 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:00:35,995 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:00:36,066 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:00:36,066 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:00:36,378 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:00:36,390 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:00:37,989 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:00:38,035 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:00:38,945 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:00:41,003 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:00:43,245 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:00:44,887 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:00:46,678 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:00:46,679 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:00:47,691 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-14 01:00:47,706 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-14 01:00:47,710 - ERROR - [pid 10013] Worker Worker(salt=8461847662, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10013) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: missing FROM-clause entry for table "customers"
LINE 27:                         OR stg.customers.latitude <> EXCLUDE...
                                    ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) missing FROM-clause entry for table "customers"
LINE 27:                         OR stg.customers.latitude <> EXCLUDE...
                                    ^

[SQL: INSERT INTO stg.sellers
    (seller_id, seller_zip_code_prefix, latitude, longitude, seller_city, seller_state)
    
SELECT
    s.seller_id,
    s.seller_zip_code_prefix,
    g.geolocation_lat as latitude,
    g.geolocation_lng as longitude,
    s.seller_city,
    s.seller_state

FROM public.sellers s

JOIN public.geolocation g
    ON s.seller_zip_code_prefix = g.geolocation_zip_code_prefix

ON CONFLICT(seller_id)
DO UPDATE SET
    seller_zip_code_prefix = EXCLUDED.seller_zip_code_prefix,
    latitude = EXCLUDED.latitude,
    longitude = EXCLUDED.longitude,
    seller_city = EXCLUDED.seller_city,
    seller_state = EXCLUDED.seller_state,

    updated_at = CASE WHEN
                        stg.sellers.seller_zip_code_prefix <> EXCLUDED.seller_zip_code_prefix
                        OR stg.customers.latitude <> EXCLUDED.latitude 
                        OR stg.customers.longitude <> EXCLUDED.longitude 
                        OR stg.sellers.seller_city <> EXCLUDED.seller_city
                        OR stg.sellers.seller_state <> EXCLUDED.seller_state
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.sellers.updated_at
                END;
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-14 01:00:47,942 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:00:47,947 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-14 01:00:47,948 - DEBUG - Asking scheduler for work...
2025-01-14 01:00:47,951 - DEBUG - Done
2025-01-14 01:00:47,951 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:00:47,952 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-14 01:00:47,952 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-14 01:00:47,952 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-14 01:00:47,953 - INFO - Worker Worker(salt=8461847662, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10013) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:00:47,954 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:02:06,188 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:02:06,587 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:02:07,488 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:02:07,529 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:02:08,835 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:02:09,456 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:02:10,449 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:02:12,229 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:02:12,264 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:02:12,638 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:02:12,639 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:02:12,671 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:02:12,673 - INFO - [pid 10031] Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) done      Extract()
2025-01-14 01:02:12,676 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:02:12,681 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:02:12,682 - DEBUG - Asking scheduler for work...
2025-01-14 01:02:12,685 - DEBUG - Pending tasks: 2
2025-01-14 01:02:12,686 - INFO - [pid 10031] Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) running   Load()
2025-01-14 01:02:12,729 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:02:15,063 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:02:15,064 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:02:15,156 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:02:15,157 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:02:15,478 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:02:15,491 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:02:17,017 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:02:17,063 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:02:17,983 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:02:20,041 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:02:22,239 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:02:23,826 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:02:25,630 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:02:25,631 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:02:32,110 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 01:02:32,130 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 01:02:32,155 - INFO - [pid 10031] Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) done      Load()
2025-01-14 01:02:32,156 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:02:32,159 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 01:02:32,160 - DEBUG - Asking scheduler for work...
2025-01-14 01:02:32,162 - DEBUG - Pending tasks: 1
2025-01-14 01:02:32,164 - INFO - [pid 10031] Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) running   Transform()
2025-01-14 01:02:32,442 - INFO - Read Transform Query - SUCCESS
2025-01-14 01:02:32,444 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:02:32,445 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 01:02:33,262 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 01:02:33,267 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 01:02:33,283 - ERROR - Transform Tables - FAILED
2025-01-14 01:02:33,286 - ERROR - [pid 10031] Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: uuid = integer
LINE 3: ON final.order_item_id = stg.order_item_id
                               ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: uuid = integer
LINE 3: ON final.order_item_id = stg.order_item_id
                               ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 01:02:33,601 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:02:33,608 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 01:02:33,609 - DEBUG - Asking scheduler for work...
2025-01-14 01:02:33,613 - DEBUG - Done
2025-01-14 01:02:33,614 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:02:33,615 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 01:02:33,616 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 01:02:33,616 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 01:02:33,617 - INFO - Worker Worker(salt=6309446167, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10031) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:02:33,619 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:06:11,200 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:06:11,494 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:06:12,346 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:06:12,388 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:06:13,713 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:06:14,466 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:06:15,676 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:06:17,332 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:06:17,367 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:06:17,710 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:06:17,711 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:06:17,736 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:06:17,738 - INFO - [pid 10057] Worker Worker(salt=4876228269, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10057) done      Extract()
2025-01-14 01:06:17,740 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:06:17,743 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:06:17,743 - DEBUG - Asking scheduler for work...
2025-01-14 01:06:17,746 - DEBUG - Pending tasks: 2
2025-01-14 01:06:17,746 - INFO - [pid 10057] Worker Worker(salt=4876228269, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10057) running   Load()
2025-01-14 01:06:17,765 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:06:19,393 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:06:19,394 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:06:19,471 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:06:19,472 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:06:19,779 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:06:19,789 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:06:21,306 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:06:21,350 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:06:22,272 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:06:24,681 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:06:26,960 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:06:28,630 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:06:30,545 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:06:30,546 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:06:32,799 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-14 01:06:32,812 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-14 01:06:32,816 - ERROR - [pid 10057] Worker Worker(salt=4876228269, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10057) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_item_id" is of type uuid but expression is of type integer
LINE 6:     order_item_id,
            ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_item_id" is of type uuid but expression is of type integer
LINE 6:     order_item_id,
            ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO stg.order_items
    (order_id, order_item_id, product_id, seller_id, shipping_limit_date, price, freight_value)
    
SELECT
    order_id,
    order_item_id,
    product_id,
    seller_id,
    shipping_limit_date,
    price,
    freight_value

FROM public.order_items

ON CONFLICT(order_id, order_item_id)
DO UPDATE SET
    product_id = EXCLUDED.product_id,
    seller_id = EXCLUDED.seller_id,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    price = EXCLUDED.price,
    freight_value = EXCLUDED.freight_value,

    updated_at = CASE WHEN
                        stg.order_items.product_id <> EXCLUDED.product_id
                        OR stg.order_items.seller_id <> EXCLUDED.seller_id
                        OR stg.order_items.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR stg.order_items.price <> EXCLUDED.price
                        OR stg.order_items.freight_value <> EXCLUDED.freight_value
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_items.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-14 01:06:33,111 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:06:33,119 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-14 01:06:33,120 - DEBUG - Asking scheduler for work...
2025-01-14 01:06:33,123 - DEBUG - Done
2025-01-14 01:06:33,124 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:06:33,125 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-14 01:06:33,125 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-14 01:06:33,126 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-14 01:06:33,128 - INFO - Worker Worker(salt=4876228269, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10057) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:06:33,130 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:09:18,193 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:09:18,596 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:09:19,408 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:09:19,449 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:09:20,678 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:09:21,377 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:09:22,524 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:09:24,043 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:09:24,095 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:09:24,557 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:09:24,559 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:09:24,609 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:09:24,612 - INFO - [pid 10078] Worker Worker(salt=4939860183, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10078) done      Extract()
2025-01-14 01:09:24,614 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:09:24,621 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:09:24,623 - DEBUG - Asking scheduler for work...
2025-01-14 01:09:24,627 - DEBUG - Pending tasks: 2
2025-01-14 01:09:24,628 - INFO - [pid 10078] Worker Worker(salt=4939860183, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10078) running   Load()
2025-01-14 01:09:24,692 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:09:26,648 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:09:26,649 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:09:26,736 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:09:26,737 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:09:27,047 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:09:27,059 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:09:28,540 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:09:28,586 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:09:29,475 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:09:31,586 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:09:33,862 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:09:35,500 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:09:37,312 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:09:37,313 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:09:40,446 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2025-01-14 01:09:40,463 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-14 01:09:40,469 - ERROR - [pid 10078] Worker Worker(salt=4939860183, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10078) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CannotCoerce: cannot cast type integer to uuid
LINE 6:     order_item_id::uuid,
                         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 255, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CannotCoerce) cannot cast type integer to uuid
LINE 6:     order_item_id::uuid,
                         ^

[SQL: INSERT INTO stg.order_items
    (order_id, order_item_id, product_id, seller_id, shipping_limit_date, price, freight_value)
    
SELECT
    order_id,
    order_item_id::uuid,
    product_id,
    seller_id,
    shipping_limit_date,
    price,
    freight_value

FROM public.order_items

ON CONFLICT(order_id, order_item_id)
DO UPDATE SET
    product_id = EXCLUDED.product_id,
    seller_id = EXCLUDED.seller_id,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    price = EXCLUDED.price,
    freight_value = EXCLUDED.freight_value,

    updated_at = CASE WHEN
                        stg.order_items.product_id <> EXCLUDED.product_id
                        OR stg.order_items.seller_id <> EXCLUDED.seller_id
                        OR stg.order_items.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR stg.order_items.price <> EXCLUDED.price
                        OR stg.order_items.freight_value <> EXCLUDED.freight_value
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_items.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 265, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-14 01:09:40,768 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:09:40,776 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-14 01:09:40,776 - DEBUG - Asking scheduler for work...
2025-01-14 01:09:40,779 - DEBUG - Done
2025-01-14 01:09:40,780 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:09:40,781 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-14 01:09:40,781 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-14 01:09:40,782 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-14 01:09:40,782 - INFO - Worker Worker(salt=4939860183, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10078) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:09:40,784 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:11:52,922 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:11:53,362 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:11:54,214 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:11:54,257 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:11:55,700 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:11:56,321 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:11:57,415 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:11:58,799 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:11:58,834 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:11:59,184 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:11:59,186 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:11:59,224 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:11:59,226 - INFO - [pid 10097] Worker Worker(salt=3760269608, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10097) done      Extract()
2025-01-14 01:11:59,228 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:11:59,232 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:11:59,232 - DEBUG - Asking scheduler for work...
2025-01-14 01:11:59,235 - DEBUG - Pending tasks: 2
2025-01-14 01:11:59,236 - INFO - [pid 10097] Worker Worker(salt=3760269608, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10097) running   Load()
2025-01-14 01:11:59,282 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:12:01,043 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:12:01,044 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:12:01,107 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:12:01,107 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:12:01,406 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:12:01,417 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:12:02,918 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:12:02,970 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:12:03,895 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:12:05,964 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:12:06,400 - ERROR - LOAD All Tables To DWH-public - FAILED
2025-01-14 01:12:06,414 - ERROR - LOAD All Tables To DWH - FAILED
2025-01-14 01:12:06,419 - ERROR - [pid 10097] Worker Worker(salt=3760269608, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10097) failed    Load()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_item_id" is of type uuid but expression is of type integer
LINE 1: ...alue) VALUES ('00010242fe8c5a6d1ba2dd792cb16214', 1, '424473...
                                                             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 209, in run
    order_items_data.to_sql('order_items',
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_item_id" is of type uuid but expression is of type integer
LINE 1: ...alue) VALUES ('00010242fe8c5a6d1ba2dd792cb16214', 1, '424473...
                                                             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO public.order_items (order_id, order_item_id, product_id, seller_id, shipping_limit_date, price, freight_value) VALUES (%(order_id__0)s, %(order_item_id__0)s, %(product_id__0)s, %(seller_id__0)s, %(shipping_limit_date__0)s, %(price__0)s, % ... 155008 characters truncated ... id__999)s, %(seller_id__999)s, %(shipping_limit_date__999)s, %(price__999)s, %(freight_value__999)s)]
[parameters: {'product_id__0': '4244733e06e7ecb4970a6e2683c13e61', 'seller_id__0': '48436dade18ac8b2bce089ec2a041202', 'price__0': 58.9, 'shipping_limit_date__0': '2017-09-19 09:45:35', 'freight_value__0': 13.29, 'order_item_id__0': 1, 'order_id__0': '00010242fe8c5a6d1ba2dd792cb16214', 'product_id__1': 'e5f2d52b802189ee658865ca93d83a8f', 'seller_id__1': 'dd7ddc04e1b6c2c614352b383efe2d36', 'price__1': 239.9, 'shipping_limit_date__1': '2017-05-03 11:05:13', 'freight_value__1': 19.93, 'order_item_id__1': 1, 'order_id__1': '00018f77f2f0320c557190d7a144bdd3', 'product_id__2': 'c777355d18b72b67abbeef9df44fd0fd', 'seller_id__2': '5b51032eddd242adc84c38acab88f23d', 'price__2': 199.0, 'shipping_limit_date__2': '2018-01-18 14:48:30', 'freight_value__2': 17.87, 'order_item_id__2': 1, 'order_id__2': '000229ec398224ef6ca0657da4fc703e', 'product_id__3': '7634da152a4610f1595efa32f14722fc', 'seller_id__3': '9d7a1d34a5052409006425275ba1c2b4', 'price__3': 12.99, 'shipping_limit_date__3': '2018-08-15 10:10:18', 'freight_value__3': 12.79, 'order_item_id__3': 1, 'order_id__3': '00024acbcdf0a6daa1e931b038114c75', 'product_id__4': 'ac6c3623068f30de03045865e4e10089', 'seller_id__4': 'df560393f3a51e74553ab94004ba5c87', 'price__4': 199.9, 'shipping_limit_date__4': '2017-02-13 13:57:51', 'freight_value__4': 18.14, 'order_item_id__4': 1, 'order_id__4': '00042b26cf59d7ce69dfabb4e55b4fd9', 'product_id__5': 'ef92defde845ab8450f9d70c526ef70f', 'seller_id__5': '6426d21aca402a131fc0a5d0960a3c90', 'price__5': 21.9, 'shipping_limit_date__5': '2017-05-23 03:55:27', 'freight_value__5': 12.69, 'order_item_id__5': 1, 'order_id__5': '00048cc3ae777c65dbb7d2a0634bc1ea', 'product_id__6': '8d4f2bb7e93e6710a28f34fa83ee7d28', 'seller_id__6': '7040e82f899a04d1b434b795a43b4617', 'price__6': 19.9, 'shipping_limit_date__6': '2017-12-14 12:10:31', 'freight_value__6': 11.85, 'order_item_id__6': 1, 'order_id__6': '00054e8431b9d7675808bcb819fb4a32', 'product_id__7': '557d850972a7d6f792fd18ae1400d9b6' ... 6900 parameters truncated ... 'order_id__992': '024d56279184e5cb942d9abf28fc14e6', 'product_id__993': '6c321ed0f47858a8650f4104771224b5', 'seller_id__993': '6560211a19b47992c3666cc44a7e94c0', 'price__993': 49.0, 'shipping_limit_date__993': '2017-09-18 13:25:31', 'freight_value__993': 7.78, 'order_item_id__993': 1, 'order_id__993': '024f8681c73783017f7cd5d5b061be3a', 'product_id__994': 'b114bf337c0626166abe574eee9e3f32', 'seller_id__994': '7c67e1448b00f6e969d365cea6b010ab', 'price__994': 146.94, 'shipping_limit_date__994': '2018-03-27 04:31:19', 'freight_value__994': 52.85, 'order_item_id__994': 1, 'order_id__994': '02504308041b6ad0a112b2b7159439ed', 'product_id__995': 'b114bf337c0626166abe574eee9e3f32', 'seller_id__995': '7c67e1448b00f6e969d365cea6b010ab', 'price__995': 146.94, 'shipping_limit_date__995': '2018-03-27 04:31:19', 'freight_value__995': 52.85, 'order_item_id__995': 2, 'order_id__995': '02504308041b6ad0a112b2b7159439ed', 'product_id__996': '0aabfb375647d9738ad0f7b4ea3653b1', 'seller_id__996': '37515688008a7a40ac93e3b2e4ab203f', 'price__996': 36.2, 'shipping_limit_date__996': '2018-07-11 14:09:58', 'freight_value__996': 18.35, 'order_item_id__996': 1, 'order_id__996': '0250c3964e5ca77ecac2db28d78aa89f', 'product_id__997': 'd7cdea99e6f50310c242d02f16f1f63c', 'seller_id__997': '966cb4760537b1404caedd472cc610a5', 'price__997': 509.0, 'shipping_limit_date__997': '2018-07-19 10:55:17', 'freight_value__997': 12.31, 'order_item_id__997': 1, 'order_id__997': '02521a906b13fc8ff9314bf88adc31ed', 'product_id__998': 'fafb498dd2f33cbb55cbd0f7d7706047', 'seller_id__998': 'ad420dd0c4f92f8af951ac24b86d0cf5', 'price__998': 89.99, 'shipping_limit_date__998': '2017-10-30 03:35:32', 'freight_value__998': 17.07, 'order_item_id__998': 1, 'order_id__998': '0252299d97791cd5879873cba920d1f7', 'product_id__999': '28b4eced95a52d9c437a4caf9d311b95', 'seller_id__999': '77530e9772f57a62c906e1c21538ab82', 'price__999': 59.9, 'shipping_limit_date__999': '2017-06-26 22:10:14', 'freight_value__999': 50.92, 'order_item_id__999': 1, 'order_id__999': '025321e4e9674e090b7bc101f9c5ab5a'}]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 237, in run
    raise Exception('Failed Load Tables To DWH-public')
Exception: Failed Load Tables To DWH-public

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/load.py", line 304, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2025-01-14 01:12:07,247 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:12:07,255 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2025-01-14 01:12:07,256 - DEBUG - Asking scheduler for work...
2025-01-14 01:12:07,260 - DEBUG - Done
2025-01-14 01:12:07,260 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:12:07,261 - DEBUG - There are 2 pending tasks possibly being run by other workers
2025-01-14 01:12:07,262 - DEBUG - There are 2 pending tasks unique to this worker
2025-01-14 01:12:07,262 - DEBUG - There are 2 pending tasks last scheduled by this worker
2025-01-14 01:12:07,263 - INFO - Worker Worker(salt=3760269608, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10097) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:12:07,265 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:27:56,506 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:27:56,984 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:27:57,723 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:27:57,765 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:27:59,040 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:27:59,647 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:28:00,633 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:28:01,882 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:28:01,916 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:28:02,234 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:28:02,236 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:28:02,259 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:28:02,262 - INFO - [pid 10142] Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) done      Extract()
2025-01-14 01:28:02,264 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:28:02,267 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:28:02,268 - DEBUG - Asking scheduler for work...
2025-01-14 01:28:02,271 - DEBUG - Pending tasks: 2
2025-01-14 01:28:02,271 - INFO - [pid 10142] Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) running   Load()
2025-01-14 01:28:02,302 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:28:04,139 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:28:04,140 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:28:04,211 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:28:04,212 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:28:04,529 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:28:04,539 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:28:06,055 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:28:06,097 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:28:06,995 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:28:09,105 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:28:11,559 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:28:13,199 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:28:15,010 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:28:15,011 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:28:20,471 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 01:28:20,488 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 01:28:20,513 - INFO - [pid 10142] Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) done      Load()
2025-01-14 01:28:20,515 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:28:20,518 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 01:28:20,519 - DEBUG - Asking scheduler for work...
2025-01-14 01:28:20,522 - DEBUG - Pending tasks: 1
2025-01-14 01:28:20,523 - INFO - [pid 10142] Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) running   Transform()
2025-01-14 01:28:20,632 - INFO - Read Transform Query - SUCCESS
2025-01-14 01:28:20,633 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:28:20,634 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 01:28:21,431 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 01:28:21,434 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 01:28:21,452 - ERROR - Transform Tables - FAILED
2025-01-14 01:28:21,456 - ERROR - [pid 10142] Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_nk" is of type uuid but expression is of type text
LINE 9:         order_nk = stg.order_id,
                           ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_nk" is of type uuid but expression is of type text
LINE 9:         order_nk = stg.order_id,
                           ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 01:28:21,664 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:28:21,672 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 01:28:21,673 - DEBUG - Asking scheduler for work...
2025-01-14 01:28:21,677 - DEBUG - Done
2025-01-14 01:28:21,678 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:28:21,679 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 01:28:21,680 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 01:28:21,681 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 01:28:21,681 - INFO - Worker Worker(salt=4086036341, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10142) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:28:21,683 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 01:32:38,381 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 01:32:38,830 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 01:32:39,689 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 01:32:39,729 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 01:32:41,345 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 01:32:42,014 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 01:32:43,142 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 01:32:45,155 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 01:32:45,192 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 01:32:45,575 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 01:32:45,576 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 01:32:45,608 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 01:32:45,611 - INFO - [pid 10165] Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) done      Extract()
2025-01-14 01:32:45,612 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:32:45,617 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 01:32:45,618 - DEBUG - Asking scheduler for work...
2025-01-14 01:32:45,624 - DEBUG - Pending tasks: 2
2025-01-14 01:32:45,625 - INFO - [pid 10165] Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) running   Load()
2025-01-14 01:32:45,659 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 01:32:47,908 - INFO - Read Extracted Data - SUCCESS
2025-01-14 01:32:47,910 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:32:48,006 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 01:32:48,008 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 01:32:48,318 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 01:32:48,331 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 01:32:49,875 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 01:32:49,923 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 01:32:50,919 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 01:32:53,159 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 01:32:55,793 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 01:32:57,469 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 01:32:59,431 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 01:32:59,432 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 01:33:07,582 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 01:33:07,596 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 01:33:07,634 - INFO - [pid 10165] Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) done      Load()
2025-01-14 01:33:07,635 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:33:07,639 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 01:33:07,639 - DEBUG - Asking scheduler for work...
2025-01-14 01:33:07,643 - DEBUG - Pending tasks: 1
2025-01-14 01:33:07,644 - INFO - [pid 10165] Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) running   Transform()
2025-01-14 01:33:07,707 - INFO - Read Transform Query - SUCCESS
2025-01-14 01:33:07,708 - INFO - Connect to DWH - SUCCESS
2025-01-14 01:33:07,709 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 01:33:08,419 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 01:33:08,422 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 01:33:08,436 - ERROR - Transform Tables - FAILED
2025-01-14 01:33:08,440 - ERROR - [pid 10165] Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_nk" is of type uuid but expression is of type text
LINE 9:         order_nk = stg.order_id,
                           ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_nk" is of type uuid but expression is of type text
LINE 9:         order_nk = stg.order_id,
                           ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 01:33:08,791 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 01:33:08,798 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 01:33:08,799 - DEBUG - Asking scheduler for work...
2025-01-14 01:33:08,803 - DEBUG - Done
2025-01-14 01:33:08,803 - DEBUG - There are no more tasks to run at this time
2025-01-14 01:33:08,804 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 01:33:08,805 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 01:33:08,806 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 01:33:08,807 - INFO - Worker Worker(salt=5199053238, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10165) was stopped. Shutting down Keep-Alive thread
2025-01-14 01:33:08,808 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:00:18,101 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:00:18,510 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:00:19,294 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:00:19,338 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:00:20,643 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:00:21,323 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:00:22,784 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:00:24,541 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:00:24,578 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:00:24,944 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:00:24,946 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:00:24,968 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:00:24,971 - INFO - [pid 10225] Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) done      Extract()
2025-01-14 02:00:24,973 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:00:24,976 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:00:24,976 - DEBUG - Asking scheduler for work...
2025-01-14 02:00:24,979 - DEBUG - Pending tasks: 2
2025-01-14 02:00:24,980 - INFO - [pid 10225] Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) running   Load()
2025-01-14 02:00:25,017 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:00:26,864 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:00:26,865 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:00:26,941 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:00:26,942 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:00:27,267 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:00:27,280 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:00:28,793 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:00:28,835 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:00:29,735 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:00:31,831 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:00:34,297 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:00:35,901 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:00:37,746 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:00:37,746 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:00:45,522 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:00:45,534 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:00:45,559 - INFO - [pid 10225] Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) done      Load()
2025-01-14 02:00:45,560 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:00:45,564 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:00:45,564 - DEBUG - Asking scheduler for work...
2025-01-14 02:00:45,567 - DEBUG - Pending tasks: 1
2025-01-14 02:00:45,568 - INFO - [pid 10225] Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) running   Transform()
2025-01-14 02:00:45,654 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:00:45,655 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:00:45,656 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:00:46,344 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:00:46,349 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:00:46,365 - ERROR - Transform Tables - FAILED
2025-01-14 02:00:46,369 - ERROR - [pid 10225] Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "product_id" is of type uuid but expression is of type text
LINE 10:         product_id = stg.product_id,
                              ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "product_id" is of type uuid but expression is of type text
LINE 10:         product_id = stg.product_id,
                              ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:00:46,654 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:00:46,662 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:00:46,663 - DEBUG - Asking scheduler for work...
2025-01-14 02:00:46,667 - DEBUG - Done
2025-01-14 02:00:46,669 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:00:46,670 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:00:46,670 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:00:46,671 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:00:46,672 - INFO - Worker Worker(salt=1654159628, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10225) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:00:46,673 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:01:59,284 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:01:59,761 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:02:00,591 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:02:00,627 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:02:01,865 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:02:02,602 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:02:03,942 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:02:05,703 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:02:05,744 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:02:06,135 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:02:06,140 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:02:06,221 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:02:06,223 - INFO - [pid 10243] Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) done      Extract()
2025-01-14 02:02:06,224 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:02:06,227 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:02:06,228 - DEBUG - Asking scheduler for work...
2025-01-14 02:02:06,230 - DEBUG - Pending tasks: 2
2025-01-14 02:02:06,231 - INFO - [pid 10243] Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) running   Load()
2025-01-14 02:02:06,256 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:02:08,186 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:02:08,187 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:02:08,257 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:02:08,257 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:02:08,565 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:02:08,580 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:02:10,057 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:02:10,106 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:02:11,004 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:02:13,063 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:02:15,551 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:02:17,159 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:02:18,986 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:02:18,987 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:02:26,837 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:02:26,851 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:02:26,872 - INFO - [pid 10243] Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) done      Load()
2025-01-14 02:02:26,874 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:02:26,877 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:02:26,878 - DEBUG - Asking scheduler for work...
2025-01-14 02:02:26,882 - DEBUG - Pending tasks: 1
2025-01-14 02:02:26,883 - INFO - [pid 10243] Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) running   Transform()
2025-01-14 02:02:26,972 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:02:26,975 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:02:26,976 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:02:27,684 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:02:27,691 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:02:27,707 - ERROR - Transform Tables - FAILED
2025-01-14 02:02:27,711 - ERROR - [pid 10243] Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "seller_nk" is of type uuid but expression is of type text
LINE 11:         seller_nk = stg.seller_id,
                             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "seller_nk" is of type uuid but expression is of type text
LINE 11:         seller_nk = stg.seller_id,
                             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:02:27,996 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:02:28,004 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:02:28,005 - DEBUG - Asking scheduler for work...
2025-01-14 02:02:28,008 - DEBUG - Done
2025-01-14 02:02:28,009 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:02:28,009 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:02:28,010 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:02:28,011 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:02:28,012 - INFO - Worker Worker(salt=7034596821, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10243) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:02:28,013 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:04:43,171 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:04:43,573 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:04:44,350 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:04:44,392 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:04:45,745 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:04:46,424 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:04:47,763 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:04:49,877 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:04:49,916 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:04:50,291 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:04:50,293 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:04:50,327 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:04:50,329 - INFO - [pid 10264] Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) done      Extract()
2025-01-14 02:04:50,331 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:04:50,335 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:04:50,336 - DEBUG - Asking scheduler for work...
2025-01-14 02:04:50,339 - DEBUG - Pending tasks: 2
2025-01-14 02:04:50,340 - INFO - [pid 10264] Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) running   Load()
2025-01-14 02:04:50,376 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:04:52,224 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:04:52,225 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:04:52,310 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:04:52,311 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:04:52,629 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:04:52,640 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:04:54,065 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:04:54,109 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:04:55,040 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:04:57,367 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:04:59,862 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:05:01,496 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:05:03,310 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:05:03,311 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:05:10,542 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:05:10,552 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:05:10,576 - INFO - [pid 10264] Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) done      Load()
2025-01-14 02:05:10,577 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:05:10,580 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:05:10,581 - DEBUG - Asking scheduler for work...
2025-01-14 02:05:10,585 - DEBUG - Pending tasks: 1
2025-01-14 02:05:10,586 - INFO - [pid 10264] Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) running   Transform()
2025-01-14 02:05:10,648 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:05:10,650 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:05:10,650 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:05:11,388 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:05:11,391 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:05:11,402 - ERROR - Transform Tables - FAILED
2025-01-14 02:05:11,407 - ERROR - [pid 10264] Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "shipping_limit_date" is of type timestamp without time zone but expression is of type text
LINE 12:         shipping_limit_date = stg.shipping_limit_date,
                                       ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "shipping_limit_date" is of type timestamp without time zone but expression is of type text
LINE 12:         shipping_limit_date = stg.shipping_limit_date,
                                       ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:05:11,616 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:05:11,625 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:05:11,626 - DEBUG - Asking scheduler for work...
2025-01-14 02:05:11,632 - DEBUG - Done
2025-01-14 02:05:11,633 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:05:11,634 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:05:11,635 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:05:11,636 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:05:11,637 - INFO - Worker Worker(salt=2869050695, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10264) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:05:11,639 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:09:48,433 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:09:48,895 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:09:49,658 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:09:49,695 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:09:50,951 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:09:51,574 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:09:52,612 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:09:53,977 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:09:54,011 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:09:54,380 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:09:54,382 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:09:54,421 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:09:54,423 - INFO - [pid 10290] Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) done      Extract()
2025-01-14 02:09:54,424 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:09:54,427 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:09:54,427 - DEBUG - Asking scheduler for work...
2025-01-14 02:09:54,429 - DEBUG - Pending tasks: 2
2025-01-14 02:09:54,430 - INFO - [pid 10290] Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) running   Load()
2025-01-14 02:09:54,460 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:09:56,384 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:09:56,385 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:09:56,461 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:09:56,462 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:09:56,777 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:09:56,789 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:09:58,417 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:09:58,462 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:09:59,380 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:10:01,508 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:10:03,954 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:10:05,629 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:10:07,431 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:10:07,432 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:10:13,802 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:10:13,813 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:10:13,835 - INFO - [pid 10290] Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) done      Load()
2025-01-14 02:10:13,836 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:10:13,839 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:10:13,840 - DEBUG - Asking scheduler for work...
2025-01-14 02:10:13,842 - DEBUG - Pending tasks: 1
2025-01-14 02:10:13,843 - INFO - [pid 10290] Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) running   Transform()
2025-01-14 02:10:13,978 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:10:13,980 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:10:13,982 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:10:14,680 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:10:14,683 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:10:14,697 - ERROR - Transform Tables - FAILED
2025-01-14 02:10:14,701 - ERROR - [pid 10290] Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "shipping_limit_date" is of type timestamp without time zone but expression is of type text
LINE 36:         stg.shipping_limit_date, 
                 ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "shipping_limit_date" is of type timestamp without time zone but expression is of type text
LINE 36:         stg.shipping_limit_date, 
                 ^
HINT:  You will need to rewrite or cast the expression.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date::timestamp,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:10:14,937 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:10:14,944 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:10:14,945 - DEBUG - Asking scheduler for work...
2025-01-14 02:10:14,947 - DEBUG - Done
2025-01-14 02:10:14,948 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:10:14,948 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:10:14,949 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:10:14,949 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:10:14,950 - INFO - Worker Worker(salt=1202022302, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10290) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:10:14,951 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:11:05,137 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:11:05,533 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:11:06,421 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:11:06,463 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:11:08,325 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:11:09,010 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:11:10,265 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:11:11,944 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:11:11,977 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:11:12,326 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:11:12,328 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:11:12,351 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:11:12,352 - INFO - [pid 10308] Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) done      Extract()
2025-01-14 02:11:12,354 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:11:12,357 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:11:12,358 - DEBUG - Asking scheduler for work...
2025-01-14 02:11:12,360 - DEBUG - Pending tasks: 2
2025-01-14 02:11:12,361 - INFO - [pid 10308] Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) running   Load()
2025-01-14 02:11:12,394 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:11:14,149 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:11:14,150 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:11:14,247 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:11:14,249 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:11:14,561 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:11:14,572 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:11:16,093 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:11:16,137 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:11:17,118 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:11:19,300 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:11:21,762 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:11:23,350 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:11:25,158 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:11:25,159 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:11:31,500 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:11:31,513 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:11:31,537 - INFO - [pid 10308] Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) done      Load()
2025-01-14 02:11:31,538 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:11:31,542 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:11:31,543 - DEBUG - Asking scheduler for work...
2025-01-14 02:11:31,546 - DEBUG - Pending tasks: 1
2025-01-14 02:11:31,547 - INFO - [pid 10308] Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) running   Transform()
2025-01-14 02:11:31,637 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:11:31,639 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:11:31,639 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:11:32,274 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:11:32,289 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:11:32,305 - ERROR - Transform Tables - FAILED
2025-01-14 02:11:32,310 - ERROR - [pid 10308] Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dim_order_items_order_item_nk_key"
DETAIL:  Key (order_item_nk)=(1) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dim_order_items_order_item_nk_key"
DETAIL:  Key (order_item_nk)=(1) already exists.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date::timestamp,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date::timestamp, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:11:32,528 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:11:32,535 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:11:32,536 - DEBUG - Asking scheduler for work...
2025-01-14 02:11:32,538 - DEBUG - Done
2025-01-14 02:11:32,539 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:11:32,540 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:11:32,541 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:11:32,542 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:11:32,542 - INFO - Worker Worker(salt=6635150081, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10308) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:11:32,544 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:15:33,125 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:15:33,564 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:15:34,445 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:15:34,485 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:15:35,883 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:15:36,565 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:15:37,493 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:15:39,735 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:15:39,771 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:15:40,183 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:15:40,185 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:15:40,236 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:15:40,239 - INFO - [pid 10334] Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) done      Extract()
2025-01-14 02:15:40,242 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:15:40,248 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:15:40,249 - DEBUG - Asking scheduler for work...
2025-01-14 02:15:40,253 - DEBUG - Pending tasks: 2
2025-01-14 02:15:40,254 - INFO - [pid 10334] Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) running   Load()
2025-01-14 02:15:40,294 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:15:42,291 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:15:42,292 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:15:42,375 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:15:42,376 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:15:42,688 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:15:42,699 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:15:44,164 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:15:44,204 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:15:45,163 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:15:47,258 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:15:49,758 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:15:51,441 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:15:53,228 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:15:53,229 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:15:59,765 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:15:59,779 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:15:59,804 - INFO - [pid 10334] Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) done      Load()
2025-01-14 02:15:59,805 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:15:59,808 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:15:59,809 - DEBUG - Asking scheduler for work...
2025-01-14 02:15:59,812 - DEBUG - Pending tasks: 1
2025-01-14 02:15:59,812 - INFO - [pid 10334] Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) running   Transform()
2025-01-14 02:15:59,874 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:15:59,876 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:15:59,877 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:16:00,752 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:16:00,771 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:16:00,786 - ERROR - Transform Tables - FAILED
2025-01-14 02:16:00,790 - ERROR - [pid 10334] Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dim_order_items_order_nk_key"
DETAIL:  Key (order_nk)=(0008288aa423d2a3f00fcb17cd7d8719) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dim_order_items_order_nk_key"
DETAIL:  Key (order_nk)=(0008288aa423d2a3f00fcb17cd7d8719) already exists.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date::timestamp,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date::timestamp, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:16:01,049 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:16:01,057 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:16:01,058 - DEBUG - Asking scheduler for work...
2025-01-14 02:16:01,061 - DEBUG - Done
2025-01-14 02:16:01,062 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:16:01,063 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:16:01,063 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:16:01,064 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:16:01,065 - INFO - Worker Worker(salt=3099668868, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10334) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:16:01,067 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:18:57,663 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:18:58,108 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:18:58,911 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:18:58,949 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:19:00,228 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:19:00,815 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:19:01,926 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:19:03,839 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:19:03,881 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:19:04,353 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:19:04,354 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:19:04,383 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:19:04,385 - INFO - [pid 10358] Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) done      Extract()
2025-01-14 02:19:04,387 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:19:04,389 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:19:04,390 - DEBUG - Asking scheduler for work...
2025-01-14 02:19:04,392 - DEBUG - Pending tasks: 2
2025-01-14 02:19:04,393 - INFO - [pid 10358] Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) running   Load()
2025-01-14 02:19:04,425 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:19:06,436 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:19:06,437 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:19:06,542 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:19:06,544 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:19:06,872 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:19:06,883 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:19:08,441 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:19:08,485 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:19:09,417 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:19:11,464 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:19:13,889 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:19:15,499 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:19:17,292 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:19:17,293 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:19:23,948 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:19:23,966 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:19:23,992 - INFO - [pid 10358] Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) done      Load()
2025-01-14 02:19:23,993 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:19:23,996 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:19:23,996 - DEBUG - Asking scheduler for work...
2025-01-14 02:19:24,001 - DEBUG - Pending tasks: 1
2025-01-14 02:19:24,001 - INFO - [pid 10358] Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) running   Transform()
2025-01-14 02:19:24,064 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:19:24,065 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:19:24,066 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:19:24,705 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:19:25,398 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:19:25,410 - ERROR - Transform Tables - FAILED
2025-01-14 02:19:25,414 - ERROR - [pid 10358] Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "dim_order_items" violates foreign key constraint "fk_order_nk"
DETAIL:  Key (order_nk)=(ffea20c7630343a6cd9e09858c1295cd) is not present in table "dim_orders".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "dim_order_items" violates foreign key constraint "fk_order_nk"
DETAIL:  Key (order_nk)=(ffea20c7630343a6cd9e09858c1295cd) is not present in table "dim_orders".

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date::timestamp,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date::timestamp, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:19:25,648 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:19:25,655 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:19:25,656 - DEBUG - Asking scheduler for work...
2025-01-14 02:19:25,660 - DEBUG - Done
2025-01-14 02:19:25,660 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:19:25,661 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:19:25,662 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:19:25,663 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:19:25,664 - INFO - Worker Worker(salt=9952026835, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10358) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:19:25,665 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2025-01-14 02:34:05,079 - INFO - ==================================STARTING EXTRACT DATA=======================================
2025-01-14 02:34:05,584 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2025-01-14 02:34:06,471 - INFO - EXTRACT 'public.customers' - SUCCESS.
2025-01-14 02:34:06,516 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2025-01-14 02:34:07,886 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2025-01-14 02:34:08,654 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2025-01-14 02:34:10,026 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2025-01-14 02:34:11,967 - INFO - EXTRACT 'public.orders' - SUCCESS.
2025-01-14 02:34:12,007 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2025-01-14 02:34:12,395 - INFO - EXTRACT 'public.products' - SUCCESS.
2025-01-14 02:34:12,395 - INFO - Extract All Tables From Sources - SUCCESS
2025-01-14 02:34:12,428 - INFO - ==================================ENDING EXTRACT DATA=======================================
2025-01-14 02:34:12,431 - INFO - [pid 10396] Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) done      Extract()
2025-01-14 02:34:12,435 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:34:12,442 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2025-01-14 02:34:12,444 - DEBUG - Asking scheduler for work...
2025-01-14 02:34:12,448 - DEBUG - Pending tasks: 2
2025-01-14 02:34:12,450 - INFO - [pid 10396] Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) running   Load()
2025-01-14 02:34:12,496 - ERROR - truncate query TRUNCATE TABLE public.customers CASCADE;
TRUNCATE TABLE public.geolocation CASCADE;
TRUNCATE TABLE public.order_items CASCADE;
TRUNCATE TABLE public.order_payments CASCADE;
TRUNCATE TABLE public.order_reviews CASCADE;
TRUNCATE TABLE public.orders CASCADE;
TRUNCATE TABLE public.product_category_name_translation CASCADE;
TRUNCATE TABLE public.products CASCADE;
TRUNCATE TABLE public.sellers CASCADE;
2025-01-14 02:34:14,495 - INFO - Read Extracted Data - SUCCESS
2025-01-14 02:34:14,496 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:34:14,580 - INFO - Truncate sources Schema in DWH - SUCCESS
2025-01-14 02:34:14,580 - INFO - ==================================STARTING LOAD DATA=======================================
2025-01-14 02:34:14,930 - INFO - LOAD 'public.geolocation' - SUCCESS
2025-01-14 02:34:14,942 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2025-01-14 02:34:16,533 - INFO - LOAD 'public.customers' - SUCCESS
2025-01-14 02:34:16,576 - INFO - LOAD 'public.sellers' - SUCCESS
2025-01-14 02:34:17,513 - INFO - LOAD 'public.products' - SUCCESS
2025-01-14 02:34:19,655 - INFO - LOAD 'public.orders' - SUCCESS
2025-01-14 02:34:22,141 - INFO - LOAD 'public.order_items' - SUCCESS
2025-01-14 02:34:23,793 - INFO - LOAD 'public.order_payments' - SUCCESS
2025-01-14 02:34:25,617 - INFO - LOAD 'public.order_reviews' - SUCCESS
2025-01-14 02:34:25,618 - INFO - LOAD All Tables To DWH-public - SUCCESS
2025-01-14 02:34:34,423 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2025-01-14 02:34:34,437 - INFO - ==================================ENDING LOAD DATA=======================================
2025-01-14 02:34:34,471 - INFO - [pid 10396] Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) done      Load()
2025-01-14 02:34:34,472 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:34:34,478 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2025-01-14 02:34:34,479 - DEBUG - Asking scheduler for work...
2025-01-14 02:34:34,483 - DEBUG - Pending tasks: 1
2025-01-14 02:34:34,484 - INFO - [pid 10396] Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) running   Transform()
2025-01-14 02:34:34,550 - INFO - Read Transform Query - SUCCESS
2025-01-14 02:34:34,552 - INFO - Connect to DWH - SUCCESS
2025-01-14 02:34:34,552 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2025-01-14 02:34:35,393 - INFO - Transform to 'final.dim_customers' - SUCCESS
2025-01-14 02:34:35,398 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2025-01-14 02:34:35,413 - ERROR - Transform Tables - FAILED
2025-01-14 02:34:35,417 - ERROR - [pid 10396] Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) failed    Transform()
Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dim_order_items_order_nk_key"
DETAIL:  Key (order_nk)=(008d9bf350ff02ed444b3452cf3f57e0) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 118, in run
    session.execute(query)
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dim_order_items_order_nk_key"
DETAIL:  Key (order_nk)=(008d9bf350ff02ed444b3452cf3f57e0) already exists.

[SQL: MERGE INTO final.dim_order_items AS final
USING stg.order_items AS stg
ON final.order_item_id = stg.order_item_id

WHEN MATCHED THEN
    UPDATE SET
        price = stg.price,
        freight_value = stg.freight_value,
        order_nk = stg.order_id,
        product_id = stg.product_id,
        seller_nk = stg.seller_id,
        shipping_limit_date = stg.shipping_limit_date::timestamp,
        updated_at = CURRENT_TIMESTAMP

WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, 
        order_item_nk, 
        price, 
        freight_value, 
        order_nk, 
        product_id, 
        seller_nk, 
        shipping_limit_date, 
        created_at, 
        updated_at
    )
    VALUES (
        gen_random_uuid(), 
        stg.order_item_id, 
        stg.price, 
        stg.freight_value, 
        stg.order_id, 
        stg.product_id, 
        stg.seller_id, 
        stg.shipping_limit_date::timestamp, 
        CURRENT_TIMESTAMP, 
        CURRENT_TIMESTAMP
    );
]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/mnt/d/coding/training/Data_Warehouse/Week_05/dataset-olist/.venvu/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/mnt/d/Coding/Training/Data_Warehouse/Week_05/dataset-olist/pipeline/transform.py", line 208, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2025-01-14 02:34:35,687 - DEBUG - 1 running tasks, waiting for next task to finish
2025-01-14 02:34:35,695 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2025-01-14 02:34:35,696 - DEBUG - Asking scheduler for work...
2025-01-14 02:34:35,699 - DEBUG - Done
2025-01-14 02:34:35,700 - DEBUG - There are no more tasks to run at this time
2025-01-14 02:34:35,701 - DEBUG - There are 1 pending tasks possibly being run by other workers
2025-01-14 02:34:35,701 - DEBUG - There are 1 pending tasks unique to this worker
2025-01-14 02:34:35,701 - DEBUG - There are 1 pending tasks last scheduled by this worker
2025-01-14 02:34:35,702 - INFO - Worker Worker(salt=1300112541, workers=1, host=DESKTOP-MDVFE7I, username=user, pid=10396) was stopped. Shutting down Keep-Alive thread
2025-01-14 02:34:35,703 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

